{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, average_precision_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from Py_Files import metric_inventory\n",
    "from Py_Files import aws_rds\n",
    "from Py_Files import credentials\n",
    "from Py_Files import data_exploration\n",
    "from Py_Files import analytics\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/joeybortfeld/Downloads/modeling_dataset_with_bankruptcy_labels.csv')\n",
    "data['fiscal_end_date'] = pd.to_datetime(data['fiscal_end_date'])\n",
    "data['constant'] = 1\n",
    "print(data.shape)\n",
    "print(data['fsym_id'].nunique())\n",
    "print(data['fiscal_end_date'].min())\n",
    "print(data['fiscal_end_date'].max())\n",
    "\n",
    "print(data['default_1'].value_counts())\n",
    "\n",
    "# build broad sector mapping\n",
    "factset_econ_sector_to_broad_dict = {   \n",
    "'Commercial Services': 'Consumer',\n",
    "'Communications': 'Consumer',\n",
    "'Consumer Durables': 'Consumer',\n",
    "'Consumer Non-Durables': 'Consumer',\n",
    "'Consumer Services': 'Consumer',\n",
    "'Distribution Services': 'Consumer',\n",
    "'Electronic Technology': 'Technology',\n",
    "'Energy Minerals': 'Industrial',\n",
    "'Finance': 'Consumer',\n",
    "'Health Services': 'Healthcare',\n",
    "'Health Technology': 'Healthcare',\n",
    "'Industrial Services': 'Industrial',\n",
    "'Miscellaneous': 'Consumer',\n",
    "'Non-Energy Minerals': 'Industrial',\n",
    "'Process Industries': 'Industrial',\n",
    "'Producer Manufacturing': 'Industrial',\n",
    "'Retail Trade': 'Consumer',\n",
    "'Technology Services': 'Technology',\n",
    "'Transportation': 'Industrial',\n",
    "'Utilities': 'Utilities',\n",
    "'@NA': 'Consumer'\n",
    "}\n",
    "\n",
    "data['factset_econ_sector_broad'] = data['factset_econ_sector'].map(factset_econ_sector_to_broad_dict)\n",
    "\n",
    "\n",
    "\n",
    "# build interaction terms\n",
    "data['ff_assets_in_usd'] = data.groupby('fsym_id')['ff_assets_in_usd'].ffill(limit=4)\n",
    "\n",
    "# convert to decile\n",
    "data['ff_assets_in_usd_decile'] = pd.qcut( data['ff_assets_in_usd'], q=100, labels=False)\n",
    "data['total_equity_to_assets_decile'] = pd.qcut(data['total_equity_to_assets'], q=100, labels=False)\n",
    "data['net_income_to_sales_decile'] = pd.qcut(data['net_income_to_sales'], q=100, labels=False)\n",
    "\n",
    "for m in ['ff_assets_in_usd_decile', 'total_equity_to_assets_decile', 'net_income_to_sales_decile']:\n",
    "    data[m] = data[m] + 1\n",
    "\n",
    "data['size_x_leverage'] = data['ff_assets_in_usd_decile'] * data['total_equity_to_assets_decile']\n",
    "data['size_x_profitability'] = data['ff_assets_in_usd_decile'] * data['net_income_to_sales_decile']\n",
    "\n",
    "data['size_x_leverage'] = pd.qcut(data['size_x_leverage'], q=100, labels=False)\n",
    "data['size_x_profitability'] = pd.qcut(data['size_x_profitability'], q=100, labels=False)\n",
    "\n",
    "print('done all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "\n",
    "{'model_number': 0, 'model_name': 'baseline model',\n",
    " 'x1_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('total_debt_to_ebitda', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x2_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('total_debt_to_ebitda', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x3_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('total_debt_to_ebitda', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x4_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('total_debt_to_ebitda', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x5_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('total_debt_to_ebitda', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'incl_sector_dummies': False,\n",
    " 'sector_dummies_var': 'factset_econ_sector',\n",
    " 'sector_var': 'factset_econ_sector', # used for intra-sector AUROC calculations, cross-sector rank correlation\n",
    " 'test_split_date_list': None,\n",
    "'write_company_outputs': False\n",
    "\n",
    " },\n",
    "\n",
    " {'model_number': 1, 'model_name': 'sector dummies + variable ratios',\n",
    " 'x1_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x2_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x3_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x4_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x5_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'incl_sector_dummies': True,\n",
    " 'sector_dummies_var': 'factset_econ_sector_broad',\n",
    " 'sector_var': 'factset_econ_sector_broad', # used for intra-sector AUROC calculations, cross-sector rank correlation\n",
    " 'test_split_date_list': None,\n",
    " 'write_company_outputs': False\n",
    " },\n",
    "\n",
    "\n",
    " {'model_number': 2, 'model_name': 'sector dummies + variable ratios + test',\n",
    " 'x1_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x2_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x3_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x4_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x5_specs': [('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'incl_sector_dummies': True,\n",
    " 'sector_dummies_var': 'factset_econ_sector_broad',\n",
    " 'sector_var': 'factset_econ_sector_broad', # used for intra-sector AUROC calculations, cross-sector rank correlation\n",
    " 'test_split_date_list': ['2020-01-01', '2019-01-01', '2018-01-01', '2017-01-01', '2016-01-01'],\n",
    "  'write_company_outputs': False\n",
    " },\n",
    "\n",
    "  {'model_number': 3, 'model_name': 'size',\n",
    " 'x1_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x2_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x3_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x4_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'x5_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('ebitda_vol', 'pct'), ('constant', 'level')],\n",
    " 'incl_sector_dummies': True,\n",
    " 'sector_dummies_var': 'factset_econ_sector_broad',\n",
    " 'sector_var': 'factset_econ_sector_broad', # used for intra-sector AUROC calculations, cross-sector rank correlation\n",
    " 'test_split_date_list': None,\n",
    "  'write_company_outputs': False\n",
    " },\n",
    "\n",
    "  {'model_number': 4, 'model_name': 'no volatility',\n",
    " 'x1_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x2_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x3_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_interest_expense', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x4_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'x5_specs': [('ff_assets_in_usd', 'pct'), ('total_equity_to_assets', 'pct'), ('net_income_to_sales', 'pct'), ('ebitda_to_net_debt', 'pct'), ('cash_to_total_debt', 'pct'), ('constant', 'level')],\n",
    " 'incl_sector_dummies': True,\n",
    " 'sector_dummies_var': 'factset_econ_sector_broad',\n",
    " 'sector_var': 'factset_econ_sector_broad', # used for intra-sector AUROC calculations, cross-sector rank correlation\n",
    " 'test_split_date_list': None,\n",
    "  'write_company_outputs': True\n",
    " },\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "write_results = True\n",
    "write_dir = '/Users/joeybortfeld/Documents/QML Solutions Data/exploratory_data/default_model_regressions/'\n",
    "write_company_outputs_dir = '/Users/joeybortfeld/Documents/QML Solutions Data/exploratory_data/default_model_company_outputs/'\n",
    "\n",
    "models_meta = []\n",
    "for this_model in model_list:\n",
    "\n",
    "    # extract model specifications\n",
    "    model_num = this_model['model_number']\n",
    "    model_name = this_model['model_name']\n",
    "\n",
    "    print(model_num, model_name)\n",
    "\n",
    "    incl_sector_dummies = this_model['incl_sector_dummies']\n",
    "    sector_dummies_var = this_model['sector_dummies_var']\n",
    "    sector_var = this_model['sector_var']\n",
    "    test_split_date_list = this_model['test_split_date_list']\n",
    "    write_company_outputs = this_model['write_company_outputs']\n",
    "\n",
    "    x_dict = {}\n",
    "    x_dict['specs'] = {}\n",
    "    x_dict['specs'][1] = this_model['x1_specs']\n",
    "    x_dict['specs'][2] = this_model['x2_specs']\n",
    "    x_dict['specs'][3] = this_model['x3_specs']\n",
    "    x_dict['specs'][4] = this_model['x4_specs']\n",
    "    x_dict['specs'][5] = this_model['x5_specs']\n",
    "\n",
    "    x_dict['base_vars'] = {}\n",
    "    x_dict['vars'] = {}\n",
    "    x_dict['vars_to_pct'] = {}\n",
    "    for i in [1,2,3,4,5]:\n",
    "        x_dict['base_vars'][i] = [x[0] for x in x_dict['specs'][i]]\n",
    "        x_dict['vars'][i] = [x[0] if x[1] == 'level' else f'{x[0]}_pct' for x in x_dict['specs'][i]]\n",
    "        x_dict['vars_to_pct'][i] = [x[0] for x in x_dict['specs'][i] if x[1] == 'pct']\n",
    "\n",
    "\n",
    "    # generate a list of all base variables across all horizons\n",
    "    all_base_vars_list = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "        all_base_vars_list.extend(x_dict['base_vars'][i])\n",
    "    all_base_vars_list = list(set(all_base_vars_list))\n",
    "\n",
    "    all_x_vars_list = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "        all_x_vars_list.extend(x_dict['vars'][i])\n",
    "    all_x_vars_list = list(set(all_x_vars_list))\n",
    "\n",
    "    all_x_to_pct_list = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "        all_x_to_pct_list.extend(x_dict['vars_to_pct'][i])\n",
    "    all_x_to_pct_list = list(set(all_x_to_pct_list))\n",
    "\n",
    "    if verbose:\n",
    "        print('x_vars:', all_x_vars_list)\n",
    "        print('all base vars:', all_base_vars_list)\n",
    "        print('vars to apply percentiles to:', all_x_to_pct_list)\n",
    "    \n",
    "    ########################################################\n",
    "    # build training dataset (and test dataset if applicable)\n",
    "    df_model = data[['fsym_id', 'fiscal_end_date', 'factset_econ_sector', 'factset_econ_sector_broad', 'ff_assets_in_usd_decile', 'bankruptcy_date',] + all_base_vars_list + ['default_1', 'default_2', 'default_3', 'default_4', 'default_5']].copy()\n",
    "\n",
    "    if incl_sector_dummies:\n",
    "        \n",
    "        # construct sector dummies\n",
    "        sector_dummies = pd.get_dummies(df_model[sector_dummies_var], prefix='sector', dtype=float, drop_first=True)\n",
    "        df_model = pd.concat([df_model, sector_dummies], axis=1)\n",
    "        print('--including sector dummies:', sector_dummies.columns)\n",
    "        print('--sector holdout:', [x for x in df_model[sector_dummies_var].unique() if not f'sector_{x}' in sector_dummies.columns])\n",
    "\n",
    "\n",
    "    # percentile calculation and transformations\n",
    "    # -- if using test data,use the 1Y train/test split date to split off a dataset for percentile calculations\n",
    "    # -- otherwise, use the full dataset\n",
    "    df_pct_set = df_model.copy()\n",
    "    mask = df_pct_set[all_base_vars_list].isna().any(axis=1)\n",
    "    df_pct_set = df_pct_set[~mask]\n",
    "    if test_split_date_list is not None:\n",
    "        df_pct_set = df_pct_set[df_pct_set['fiscal_end_date'] < pd.to_datetime(test_split_date_list[0])].copy()\n",
    "\n",
    "    # apply percentile transfomations to x-variables\n",
    "    cutpoints_dict = {}\n",
    "    if len(all_x_to_pct_list) > 0:\n",
    "        \n",
    "        for x in all_x_to_pct_list:\n",
    "            if verbose:\n",
    "                print(f'--calculating cutpoints for {x}')\n",
    "            this_boundaries = analytics.calculate_percentile_bins(df_pct_set, column=x, num_bins=1000)\n",
    "            cutpoints_dict[x] = this_boundaries\n",
    "\n",
    "            df_model[f'{x}_pct'] = analytics.assign_to_bins(df_model, column=x, boundaries=this_boundaries)\n",
    "\n",
    "    # calculate variable correlations\n",
    "    df_correlation = analytics.calculate_variable_correlations(df_model, all_x_vars_list)\n",
    "    if write_results:\n",
    "        df_correlation.to_csv(write_dir + f'model_variable_correlations_{model_num}.csv', index=False)\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    # iterate over default horizons\n",
    "    all_stats = []\n",
    "    intra_sector_auroc = []\n",
    "    sector_means =[]\n",
    "    cross_sector_rank_corr = []\n",
    "    coeff_dict = {}\n",
    "\n",
    "    for t, y_var in enumerate(['default_1', 'default_2', 'default_3', 'default_4', 'default_5']):\n",
    "\n",
    "        print('--regression on', y_var)\n",
    "\n",
    "        this_test_split_date = test_split_date_list[t] if test_split_date_list is not None else None\n",
    "\n",
    "        # build the train / test dataset\n",
    "        df_train = df_model.copy()\n",
    "        if test_split_date_list is not None:\n",
    "            df_test = df_model[df_model['fiscal_end_date'] > pd.to_datetime(this_test_split_date)].copy()\n",
    "            df_train = df_model[df_model['fiscal_end_date'] <= pd.to_datetime(this_test_split_date)].copy()\n",
    "\n",
    "        # drop rows with incomplete x-variables from train data\n",
    "        mask = df_train[all_base_vars_list].isna().any(axis=1)\n",
    "        df_train = df_train[~mask]\n",
    "\n",
    "        # get the x-variables for the current horizon\n",
    "        x_vars = x_dict['vars'][int(y_var[-1])]\n",
    "        x_vars_ratios_only = x_vars.copy()\n",
    "        if incl_sector_dummies:\n",
    "            x_vars.extend(sector_dummies.columns.tolist())\n",
    "\n",
    "        coeff_dict[y_var] = {}\n",
    "\n",
    "        # subset the training data to ignore default=-1 during the model regression\n",
    "        dff = df_train[df_train[y_var] != -1].copy()\n",
    "\n",
    "        # estimation dataset statistics\n",
    "        nobs = dff.shape[0]\n",
    "        n_fsym_id = dff['fsym_id'].nunique()\n",
    "        n_default = dff[y_var].sum()\n",
    "        n_fsym_id_default = dff[dff[y_var]==1]['fsym_id'].nunique()\n",
    "        \n",
    "        # regression\n",
    "        X = dff[x_vars]\n",
    "        y = dff[y_var].values\n",
    "        logit_model = sm.Logit(y, X, maxiter=100)\n",
    "        result = logit_model.fit()\n",
    "        \n",
    "        # collect estimated regression parameters and performance\n",
    "        coefs = result.params\n",
    "        tstats = result.tvalues\n",
    "        pvals = result.pvalues\n",
    "\n",
    "        # calculate AUROC using the estimation dataset\n",
    "        y_pred = result.predict(X)\n",
    "        dff['y_pred'] = y_pred\n",
    "        auroc = roc_auc_score(np.array(y), np.array(y_pred))\n",
    "        \n",
    "        # save in-sample (estimation) statistics as table\n",
    "        stats = pd.DataFrame({'x': x_vars, 'coef': coefs, 'tstat': tstats, 'pval': pvals})\n",
    "        stats['auroc'] = auroc\n",
    "        stats['y'] = y_var\n",
    "        stats['nobs'] = nobs\n",
    "        stats['n_fsym_id'] = n_fsym_id\n",
    "        stats['n_default'] = n_default\n",
    "        stats['n_fsym_default'] = n_fsym_id_default\n",
    "        stats['mean_y'] = y.mean()\n",
    "\n",
    "        # order variables by type\n",
    "        stats['base_var'] = stats['x']\n",
    "        stats['base_var'] = stats['base_var'].map(lambda x: x.replace('_pct', ''))\n",
    "        \n",
    "        stats['var_type'] = np.NaN\n",
    "        mask1 = stats['base_var'].str.contains('sector_')\n",
    "        mask2 = stats['base_var'].str.contains('constant')\n",
    "        stats.loc[mask1 | mask2, 'var_type'] = 'dummy'\n",
    "\n",
    "        mask3 = stats['var_type'].isna()\n",
    "        stats.loc[mask3, 'var_type'] = stats.loc[mask3, 'base_var'].map(lambda x: metric_inventory.display_name_dict[x]['category'])\n",
    "\n",
    "        order_dict = {'size':0, 'leverage': 1, 'coverage': 2, 'profitability': 3, 'liquidity': 4, 'volatility': 5, 'interaction': 6, 'dummy': 7}\n",
    "        stats['var_type_order'] = stats['var_type'].map(lambda x: order_dict[x])\n",
    "        stats = stats.sort_values(by=['var_type_order', 'x'])\n",
    "        \n",
    "        # calculate pseudo-weights\n",
    "        X_stds = X.std()\n",
    "        stats['x_sd'] = stats['x'].map(lambda x: X_stds[x])\n",
    "        stats['weight'] = abs(stats['coef']) * stats['x_sd']\n",
    "        total_non_sector_weight = stats[~stats['x'].str.contains('sector_')]['weight'].sum()\n",
    "        stats['weight'] = stats['weight'] / total_non_sector_weight\n",
    "\n",
    "        mask = stats['x'].str.contains('sector_')\n",
    "        stats.loc[mask, 'weight'] = np.NaN\n",
    "\n",
    "        all_stats.append(stats)\n",
    "\n",
    "        # collect regression coefficients for later calculations (out-of-estimation dataset) and save into dictionary\n",
    "        for i, var in enumerate(x_vars):\n",
    "            coeff_dict[y_var][var] = coefs[i]\n",
    "\n",
    "        # calculate in-sample intra-sector auroc\n",
    "        sector_list = ['All'] + list(dff[sector_var].unique()) \n",
    "        sector_auroc_list = []\n",
    "        for this_sector in sector_list:\n",
    "\n",
    "            if this_sector == 'All':\n",
    "                dff_sector = dff.copy()\n",
    "            else:\n",
    "                dff_sector = dff[dff[sector_var] == this_sector].copy()\n",
    "\n",
    "            if dff_sector[y_var].sum() > 0:\n",
    "                auroc = roc_auc_score(dff_sector[y_var], dff_sector['y_pred'])\n",
    "                sector_auroc_list.append([this_sector, auroc])\n",
    "        \n",
    "        sector_auroc = pd.DataFrame(sector_auroc_list, columns=['sector', 'auroc'])\n",
    "        sector_auroc = sector_auroc.rename(columns={'auroc': y_var + '_auroc'})\n",
    "        sector_auroc = sector_auroc.set_index('sector')\n",
    "        intra_sector_auroc.append(sector_auroc)\n",
    "\n",
    "        # calculate ecological inter-sector rank correlation\n",
    "        df_sector = dff.copy()\n",
    "        df_sector= df_sector.groupby(sector_var, as_index=False)[[y_var, 'y_pred']].mean()\n",
    "\n",
    "        sector_rank_corr = df_sector[[y_var, 'y_pred']].corr(method='spearman').iloc[0][1]\n",
    "        cross_sector_rank_corr.append([y_var, sector_rank_corr])\n",
    "\n",
    "        df_sector = df_sector.rename(columns={y_var: 'y_actual'})\n",
    "        df_sector['t'] = y_var[-1]   \n",
    "        sector_means.append(df_sector)\n",
    "\n",
    "\n",
    "    # output 1 - regression statistics\n",
    "    all_stats = pd.concat(all_stats)\n",
    "    all_stats['timestamp'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    all_stats['model_number'] = model_num\n",
    "    all_stats['model_name'] = model_name\n",
    "    all_stats = all_stats[['model_number', 'model_name', 'timestamp', 'y', 'var_type', 'var_type_order', \n",
    "                           'x', 'coef', 'tstat', 'pval', 'weight', 'auroc', \n",
    "                           'nobs', 'n_fsym_id', 'n_default', 'n_fsym_default', 'mean_y']]\n",
    "    all_stats = all_stats.reset_index(drop=True)\n",
    "    if write_results:\n",
    "        all_stats.to_csv(write_dir + f'model_regression_summary_{model_num}.csv', index=False)\n",
    "\n",
    "\n",
    "    # output 2 - AUROC default/non-default discrimination by sector\n",
    "    intra_sector_auroc = pd.concat(intra_sector_auroc, axis=1) \n",
    "    intra_sector_auroc['average'] = intra_sector_auroc.mean(axis=1)\n",
    "    intra_sector_auroc = intra_sector_auroc.reset_index(drop=False)\n",
    "    intra_sector_auroc = intra_sector_auroc.sort_values(by='average', ascending=False)\n",
    "    if write_results:\n",
    "        intra_sector_auroc.to_csv(write_dir + f'model_intra_sector_auroc_{model_num}.csv', index=False)\n",
    "\n",
    "    sector_means = pd.concat(sector_means, axis=0)\n",
    "    sector_means = sector_means.rename(columns={sector_var: 'sector'})\n",
    "    if write_results:\n",
    "        sector_means.to_csv(write_dir + f'model_sector_means_{model_num}.csv', index=False)\n",
    "\n",
    "    cross_sector_rank_corr = pd.DataFrame(cross_sector_rank_corr, columns=['y', 'spearman'])\n",
    "    if write_results:\n",
    "        cross_sector_rank_corr.to_csv(write_dir + f'model_cross_sector_rank_corr_{model_num}.csv', index=False)\n",
    "\n",
    "    # apply model to full dataset (including cumulative pd calculations)\n",
    "    df = df_train.copy()\n",
    "    df = analytics.apply_model(df, coeff_dict, cutpoints_dict)\n",
    "\n",
    "    # build calibration curve\n",
    "    df_calibration_curve = analytics.build_calibration_curve(data=df, n_bins=500)\n",
    "    if write_results:\n",
    "        df_calibration_curve.to_csv(write_dir + f'model_calibration_curve_{model_num}.csv', index=False)\n",
    "\n",
    "    # build confusion matrix derivative metrics: including TPR, FPR, Precision, Recall, F1,\n",
    "    df_confusion_summary = analytics.confusion_matrix_metrics(df)\n",
    "    if write_results:\n",
    "        df_confusion_summary.to_csv(write_dir + f'model_confusion_summary_{model_num}.csv', index=False)\n",
    "\n",
    "    # calculate out-of-sample performance\n",
    "    if test_split_date_list is not None:\n",
    "        df_test = analytics.apply_model(df_test, coeff_dict, cutpoints_dict)\n",
    "        \n",
    "        test_auroc = []\n",
    "        for i in [1,2,3,4,5]:\n",
    "            temp = df_test[df_test[f'default_{i}'] != -1]\n",
    "            auroc = roc_auc_score(np.array(temp[f'default_{i}']), np.array(temp[f'pd_{i}']))\n",
    "            n_default_test = temp[f'default_{i}'].sum()\n",
    "            n_fsym_id_default_test = temp[temp[f'default_{i}']==1]['fsym_id'].nunique()\n",
    "            test_auroc.append([f'default_{i}', auroc, test_split_date_list[i-1], n_default_test, n_fsym_id_default_test])\n",
    "\n",
    "        test_auroc = pd.DataFrame(test_auroc, columns=['y', 'auroc_test', 'test_split_date', 'n_default_test', 'n_fsym_id_default_test'])\n",
    "        test_auroc = test_auroc.set_index('y')\n",
    "\n",
    "        if write_results:\n",
    "            all_stats = all_stats.merge(test_auroc, how='left', on='y')\n",
    "            all_stats.to_csv(write_dir + f'model_regression_summary_{model_num}.csv', index=False)\n",
    "        \n",
    "\n",
    "    if write_company_outputs:\n",
    "\n",
    "        if not os.path.exists(write_company_outputs_dir + f'model_output_{model_num}'):\n",
    "            os.makedirs(write_company_outputs_dir + f'model_output_{model_num}')\n",
    "\n",
    "        print('writing company outputs')\n",
    "        for fsym in tqdm.tqdm(df['fsym_id'].unique()):\n",
    "            temp = df[df['fsym_id'] == fsym].copy()\n",
    "            keep_cols = ['fiscal_end_date', 'bankruptcy_date', 'cumulative_pd_5']\n",
    "            keep_cols.extend(all_x_vars_list)\n",
    "            temp = temp[keep_cols]\n",
    "            temp.to_csv(write_company_outputs_dir + f'model_output_{model_num}/{fsym}.csv', index=False)\n",
    "\n",
    "    models_meta.append([model_num, model_name])\n",
    "\n",
    "\n",
    "models_meta = pd.DataFrame(models_meta, columns=['model_number', 'model_name'])\n",
    "models_meta.to_csv(write_dir + f'models_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['default_5'] != -1]['default_5'].mean() * 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.groupby(by='fsym_id', as_index=False)['cumulative_pd_5'].max()\n",
    "temp.to_csv('/Users/joeybortfeld/Downloads/max_pd_by_fsym.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['fiscal_end_date'] > pd.to_datetime('2024-06-30')]\n",
    "temp = temp.sort_values(by=['fsym_id', 'fiscal_end_date'])\n",
    "temp = temp.drop_duplicates(subset='fsym_id', keep='last')\n",
    "temp = temp[['fsym_id', 'cumulative_pd_5']]\n",
    "temp.to_csv('/Users/joeybortfeld/Downloads/most_recent_pd_by_fsym.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['fsym_id'] == 'K7TZB5-R']\n",
    "print(temp['cumulative_pd_5'].max())\n",
    "temp.set_index('fiscal_end_date')[['cumulative_pd_5']].plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find high PD but no default\n",
    "temp = df.copy()\n",
    "temp = temp[temp['default_1'] == 1]\n",
    "defaulted_fsym_list = temp['fsym_id'].unique().tolist()\n",
    "\n",
    "temp = df[~df['fsym_id'].isin(defaulted_fsym_list)]\n",
    "temp = temp.sort_values(by='pd_1', ascending=False)\n",
    "temp[['fsym_id', 'fiscal_end_date', 'ff_assets_in_usd_decile', 'pd_1', 'default_1']]\n",
    "\n",
    "temp2 = data[['fsym_id', 'name1', 'name2', 'bankruptcy_date']].copy()\n",
    "temp2 = temp2.drop_duplicates(subset='fsym_id', keep='first')\n",
    "temp = temp.merge(temp2, how='left', on='fsym_id')\n",
    "\n",
    "temp = temp.drop_duplicates(subset='fsym_id', keep='first')\n",
    "temp = temp[temp['pd_1'] > 0.03]\n",
    "\n",
    "temp[['fsym_id','name1', 'name2', 'fiscal_end_date', 'bankruptcy_date', 'ff_assets_in_usd_decile', 'pd_1', 'default_1']].to_csv('/Users/joeybortfeld/Downloads/high_pd_no_default.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('/Users/joeybortfeld/Documents/QML Solutions Data/universe_and_traits/bankruptcy_data.csv')\n",
    "\n",
    "temp['entity_country_hq'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsym_list = ['DG71WP-R', 'DG885T-R', 'C5CPSB-R']\n",
    "for fsym in fsym_list:\n",
    "    df[df['fsym_id'] == fsym].to_csv(f'/Users/joeybortfeld/Downloads/model_output_{model_num}_{fsym}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_test[df_test['default_1'] != -1]\n",
    "temp['default_1'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset\n",
    "y_true = np.array([0, 1, 1, 0, 1, 0, 0, 1, 0, 1])  # True binary labels\n",
    "y_pred_probs = np.array([0.05, 0.9, 0.8, 0.4, 0.7, 0.2, 0.3, 0.85, 0.1, 0.95])  # Predicted probabilities\n",
    "\n",
    "temp = df.copy()\n",
    "temp = temp[temp['default_1'] != -1]\n",
    "temp = temp[temp['pd_1'].notnull()]\n",
    "temp = temp.reset_index(drop=True)\n",
    "\n",
    "y_true = temp['default_1']\n",
    "y_pred_probs = temp['pd_1']\n",
    "\n",
    "# Step 1: Sort by predicted probabilities\n",
    "sorted_indices = np.argsort(y_pred_probs)[::-1]\n",
    "sorted_y_true = y_true[sorted_indices]\n",
    "\n",
    "# Step 2: Calculate cumulative gains\n",
    "cumulative_gain = np.cumsum(sorted_y_true) / np.sum(sorted_y_true)\n",
    "\n",
    "# Step 3: Population percentages\n",
    "population_percentage = np.linspace(0, 1, len(cumulative_gain))\n",
    "\n",
    "# Step 4: Calculate Area Under Gain Curve (AUGC)\n",
    "gain_auc = auc(population_percentage, cumulative_gain)\n",
    "\n",
    "# Step 5: Plot Gain Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(population_percentage, cumulative_gain, label=f'AUGC = {gain_auc:.2f}', color='blue', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Model')  # Baseline for random performance\n",
    "plt.xlabel('Population Percentage')\n",
    "plt.ylabel('Cumulative Gain')\n",
    "plt.title('Gain Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Output AUGC\n",
    "print(f\"Area Under Gain Curve (AUGC): {gain_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calibration_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "temp = '''[(\"total_equity_to_assets\", \"pct\"), (\"net_income_to_sales\", \"pct\"), (\"total_debt_to_ebitda\", \"pct\"), (\"cash_to_total_debt\", \"pct\"), (\"constant\", \"level\")]'''\n",
    "\n",
    "# parse the string into a list of tuples\n",
    "x_specs = ast.literal_eval(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import mixedlm\n",
    "from statsmodels.genmod.families import Binomial\n",
    "\n",
    "endog = dff['default_1']\n",
    "exog = dff[['total_equity_to_assets_pct', 'net_income_to_sales_pct', 'total_debt_to_ebitda_pct', 'cash_to_total_debt_pct', 'constant']]\n",
    "groups = dff['factset_econ_sector'].astype('category').cat.codes\n",
    "\n",
    "# Fit the model\n",
    "model = sm.MixedLM(endog, exog, groups=groups, family=Binomial())\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'default': [0, 1, 0, 1, 0, 1, 1, 0],\n",
    "    'x1': [0.5, 1.3, 2.1, 1.2, 0.7, 1.8, 2.2, 1.0],\n",
    "    'x2': [1.1, 2.3, 1.8, 2.0, 1.5, 2.1, 1.9, 1.3],\n",
    "    'x3': [0.7, 0.9, 1.5, 1.1, 1.0, 0.8, 1.4, 0.6],\n",
    "    'x4': [2.0, 1.9, 2.5, 2.3, 1.8, 2.4, 2.2, 1.7],\n",
    "    'sector': ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D']\n",
    "})\n",
    "\n",
    "# Encode 'sector' as a categorical variable\n",
    "data['sector'] = data['sector'].astype('category')\n",
    "\n",
    "# Fit the logistic regression model with random effects\n",
    "# Use GLM or an equivalent\n",
    "from statsmodels.genmod.families import Binomial\n",
    "\n",
    "# Prepare the data\n",
    "endog = data['default']  # Binary response\n",
    "exog = sm.add_constant(data[['x1', 'x2', 'x3', 'x4']])  # Predictors with constant\n",
    "groups = data['sector'].cat.codes  # Random effects grouping\n",
    "\n",
    "# Fit the model\n",
    "model = sm.MixedLM(endog, exog, groups=groups, family=Binomial())\n",
    "result = model.fit()\n",
    "\n",
    "# Summary of results\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import mixedlm\n",
    "from statsmodels.genmod.families import Binomial\n",
    "\n",
    "endog = dff['default_1']\n",
    "exog = dff[['total_equity_to_assets_pct', 'net_income_to_sales_pct', 'total_debt_to_ebitda_pct', 'cash_to_total_debt_pct', 'constant']]\n",
    "groups = dff['factset_econ_sector'].astype('category').cat.codes\n",
    "\n",
    "# Fit the model\n",
    "model = sm.MixedLM(endog, exog, groups=groups, family=Binomial())\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_cumulative_default_probability(data, coeff_dict):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # calculate the probability of default at each horizon\n",
    "    for y_var in ['default_1', 'default_2', 'default_3', 'default_4', 'default_5']:\n",
    "\n",
    "        t = int(y_var[-1])\n",
    "\n",
    "        df[f'pd_{t}'] = 0\n",
    "        for var, coeff in coeff_dict[y_var].items():\n",
    "            df[f'pd_{t}'] += df[var] * coeff\n",
    "\n",
    "        df[f'pd_{t}'] = np.exp(df[f'pd_{t}']) / (1 + np.exp(df[f'pd_{t}']))\n",
    "\n",
    "    # calculate the cumulative probability of default at each horizon\n",
    "    df[f'cumulative_pd_1'] = df[f'pd_1']\n",
    "    df[f'cumulative_pd_2'] = df['cumulative_pd_1'] + (1 - df['cumulative_pd_1']) * df[f'pd_2']\n",
    "    df[f'cumulative_pd_3'] = df['cumulative_pd_2'] + (1 - df['cumulative_pd_2']) * df[f'pd_3']\n",
    "    df[f'cumulative_pd_4'] = df['cumulative_pd_3'] + (1 - df['cumulative_pd_3']) * df[f'pd_4']\n",
    "    df[f'cumulative_pd_5'] = df['cumulative_pd_4'] + (1 - df['cumulative_pd_4']) * df[f'pd_5']\n",
    "\n",
    "    return df\n",
    "\n",
    "df = data.copy()\n",
    "df = calculate_cumulative_default_probability(df_train, coeff_dict)\n",
    "temp = df[df['fsym_id'] == 'DG71WP-R'].set_index('fiscal_end_date')[['cumulative_pd_1', 'cumulative_pd_2', 'cumulative_pd_3', 'cumulative_pd_4', 'cumulative_pd_5']]\n",
    "temp[['cumulative_pd_1', 'cumulative_pd_5']].plot(figsize=(10,5))\n",
    "temp.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "N = 2000  # Number of data points\n",
    "x1 = np.random.normal(0, 3, N) * 10\n",
    "x2 = np.random.normal(0, 2, N) * 5\n",
    "x3 = np.random.rand(N) * 6\n",
    "\n",
    "B1_true = 2.0\n",
    "B2_true = 3.45\n",
    "B3_true = -7.221\n",
    "B0_true = -1.76\n",
    "sigma_true = 20.0  # Noise standard deviation\n",
    "y = B0_true + B1_true * x1 + B2_true * x2 + B3_true * x3 + np.random.normal(0, sigma_true, N)\n",
    "\n",
    "# Combine features\n",
    "X = np.column_stack((np.ones(N), x1, x2, x3))  # Add intercept column\n",
    "features = [\"Intercept\", \"x1\", \"x2\", \"x3\"]\n",
    "\n",
    "# plot x1,x2,x3 versus y\n",
    "plt.scatter(x1, y)\n",
    "plt.scatter(x2, y)\n",
    "plt.scatter(x3, y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define prior, likelihood, and posterior\n",
    "def log_prior(B):\n",
    "    \"\"\"Log-prior: Assume independent normal priors for coefficients.\"\"\"\n",
    "    return -0.5 * np.sum(B**2 / 100**2)  # N(0, 100^2)\n",
    "\n",
    "def log_likelihood(B, X, y):\n",
    "    \"\"\"Log-likelihood: Gaussian noise.\"\"\"\n",
    "    y_hat = X @ B\n",
    "    residuals = y - y_hat\n",
    "    return -0.5 * np.sum((residuals / sigma_true) ** 2)\n",
    "\n",
    "def log_posterior(B, X, y):\n",
    "    \"\"\"Log-posterior is the sum of log-prior and log-likelihood.\"\"\"\n",
    "    return log_prior(B) + log_likelihood(B, X, y)\n",
    "\n",
    "# MCMC Metropolis-Hastings\n",
    "def metropolis_hastings(X, y, n_samples=5000, step_size=0.1):\n",
    "    \"\"\"Metropolis-Hastings sampler.\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    samples = np.zeros((n_samples, n_features))\n",
    "    current_B = np.zeros(n_features)  # Start at prior mean (0)\n",
    "    current_log_posterior = log_posterior(current_B, X, y)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Propose new coefficients\n",
    "        proposed_B = current_B + np.random.normal(0, step_size, n_features)\n",
    "        proposed_log_posterior = log_posterior(proposed_B, X, y)\n",
    "        \n",
    "        # Compute acceptance probability\n",
    "        acceptance_prob = np.exp(proposed_log_posterior - current_log_posterior)\n",
    "        \n",
    "        # Accept or reject\n",
    "        if np.random.rand() < acceptance_prob:\n",
    "            current_B = proposed_B\n",
    "            current_log_posterior = proposed_log_posterior\n",
    "        \n",
    "        samples[i] = current_B\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Run MCMC\n",
    "n_samples = 10000\n",
    "samples = metropolis_hastings(X, y, n_samples=n_samples, step_size=0.5)\n",
    "\n",
    "# Analyze Results\n",
    "burn_in = 2000  # Discard initial samples (burn-in)\n",
    "posterior_samples = samples[burn_in:]\n",
    "means = np.mean(posterior_samples, axis=0)\n",
    "\n",
    "print(\"Posterior Means:\")\n",
    "for feature, mean in zip(features, means):\n",
    "    print(f\"{feature}: {mean:.3f}\")\n",
    "\n",
    "# Plot posterior distributions\n",
    "fig, axes = plt.subplots(len(features), 1, figsize=(8, 12))\n",
    "for i, feature in enumerate(features):\n",
    "    axes[i].hist(posterior_samples[:, i], bins=50, density=True)\n",
    "    axes[i].set_title(f\"Posterior Distribution of {feature}\")\n",
    "    axes[i].axvline(means[i], color=\"red\", linestyle=\"--\", label=\"Mean\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
