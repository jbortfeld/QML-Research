{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import tqdm\n",
    "import os\n",
    "from Py_Files import credentials\n",
    "from Py_Files import factset_api\n",
    "from Py_Files import financial_modeling_prep as fmp\n",
    "from Py_Files import qml_equity_ratios\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get benchmark data\n",
    "df_sp500 = fmp.download_stock_returns(ticker='SP500', start_date='1990-01-01')\n",
    "df_sp500['date'] = pd.to_datetime(df_sp500['date'])\n",
    "df_sp500 = df_sp500[['date', 'total_return', 'adjClose']]\n",
    "df_sp500.columns = ['date', 'benchmark_return', 'benchmark_price']\n",
    "df_sp500['benchmark'] = 'SP500'\n",
    "\n",
    "df_stoxx = fmp.download_stock_returns(ticker='STOXX', start_date='1990-01-01')\n",
    "df_stoxx['date'] = pd.to_datetime(df_stoxx['date'])\n",
    "df_stoxx = df_stoxx[['date', 'total_return', 'adjClose']]\n",
    "df_stoxx.columns = ['date', 'benchmark_return', 'benchmark_price']\n",
    "df_stoxx['benchmark'] = 'STOXX'\n",
    "\n",
    "df_nikkei = fmp.download_stock_returns(ticker='NIKKEI', start_date='1990-01-01')\n",
    "df_nikkei['date'] = pd.to_datetime(df_nikkei['date'])\n",
    "df_nikkei = df_nikkei[['date', 'total_return', 'adjClose']]\n",
    "df_nikkei.columns = ['date', 'benchmark_return', 'benchmark_price']\n",
    "df_nikkei['benchmark'] = 'NIKKEI'\n",
    "\n",
    "df_benchmarks = pd.concat([df_sp500, df_stoxx, df_nikkei])\n",
    "\n",
    "for bench in df_benchmarks['benchmark'].unique():\n",
    "\n",
    "    if bench=='STOXX':\n",
    "        df_benchmarks[df_benchmarks['benchmark'] == bench].set_index('date')['benchmark_price'].plot(secondary_y=True)\n",
    "    else:\n",
    "        df_benchmarks[df_benchmarks['benchmark'] == bench].set_index('date')['benchmark_price'].plot()\n",
    "\n",
    "df_benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmarks[df_benchmarks['benchmark'] == 'STOXX'].set_index('date')['benchmark_return'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://financialmodelingprep.com/api/v3/symbol/available-indexes'\n",
    "params = {\n",
    "        \"apikey\": 'PEPnFg1Hwwgd0zkhMmuiI8DwYC2qOq7P',\n",
    "        \"from\": '2000-01-01'  # Set the start date to 1990-01-01\n",
    "    }\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(url, params=params)\n",
    "print(response.status_code)\n",
    "if response.status_code != 200:\n",
    "    print('failed to get data')\n",
    "    fadfdsf\n",
    "\n",
    "data = response.json()\n",
    "pd.DataFrame(data).to_csv('/Users/joeybortfeld/Downloads/fmp_indexes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "this_fsym = 'K5JZYK-R'\n",
    "make_plots = True\n",
    "temp = qml_equity_ratios.merge_equity_data(fsym_id=this_fsym)\n",
    "temp['fsym_id'] = this_fsym\n",
    "his_country = fsym_to_country_dict[this_fsym]\n",
    "this_region = country_to_region_dict[this_country]\n",
    "this_benchmark = region_to_benchmark_dict[this_region]\n",
    "print(this_benchmark)\n",
    "\n",
    "temp = qml_equity_ratios.combine_benchmark_data(temp, df_benchmarks, benchmark=this_benchmark)\n",
    "\n",
    "\n",
    "temp1a = qml_equity_ratios.calc_capm(temp, trailing_periods_list=[182,365], frequency='ME', outlier_drops=4, downside_only=False, exponential_weighting=(False, 0.99))\n",
    "temp1b = qml_equity_ratios.calc_capm(temp, trailing_periods_list=[182,365], frequency='ME', outlier_drops=4, downside_only=True, exponential_weighting=(False, 0.99))\n",
    "\n",
    "temp1a = qml_equity_ratios.calc_capm(temp, trailing_periods_list=[182,365], frequency='ME', outlier_drops=4, downside_only=False, exponential_weighting=(False, 0.99))\n",
    "temp1b = qml_equity_ratios.calc_capm(temp, trailing_periods_list=[182,365], frequency='ME', outlier_drops=4, downside_only=True, exponential_weighting=(False, 0.99))\n",
    "temp2 = qml_equity_ratios.calc_rolling_returns(temp)\n",
    "temp3 = qml_equity_ratios.calc_drawdown(temp)\n",
    "temp4 = qml_equity_ratios.calc_downside_volatility(temp)\n",
    "temp5 = qml_equity_ratios.calc_ulcer_index(temp)\n",
    "\n",
    "\n",
    "if make_plots:\n",
    "    if temp.shape[0] > 0:\n",
    "        \n",
    "        fig, axes = plt.subplots(figsize=(15, 14), ncols=3, nrows=3)\n",
    "        temp.set_index('date')[['price', 'price_split']].plot(ax=axes[0][0])\n",
    "        temp.set_index('date')[['market_cap', 'market_cap_split']].plot(ax=axes[0][1])\n",
    "\n",
    "        temp1a.set_index('date')[['capm_idio_vol_182', 'capm_return_vol_182']].plot(ax=axes[1][0], title='CAPM')\n",
    "        temp1b.set_index('date')[['capm_idio_vol_down_182', 'capm_return_vol_down_182']].plot(ax=axes[1][1], title='Downside Only CAPM')\n",
    "\n",
    "        temp2.set_index('date')[['return_6', 'return_12']].plot(ax=axes[1][2])\n",
    "\n",
    "        temp3.set_index('date')[['drawdown_128', 'drawdown_252']].plot(ax=axes[2][0])\n",
    "        temp4.set_index('date')[['downside_vol_128', 'downside_vol_252']].plot(ax=axes[2][1])\n",
    "        temp5.set_index('date')[['ulcer_index_128', 'ulcer_index_252']].plot(ax=axes[2][2])\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        temp[temp['date'] > pd.to_datetime('2005-12-25')][['date', 'price', 'price_split', 'market_cap', 'market_cap_split', 'total_outstanding']].tail(40)\n",
    "\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/shares/K5JZYK-R.csv')\n",
    "temp[temp['totalOutstanding'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all applicable fsym_ids\n",
    "\n",
    "# excel add-in data\n",
    "excel_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/excel_addin_download/')\n",
    "excel_fsyms = [i.split('_')[0] for i in excel_fsyms]\n",
    "print(len(excel_fsyms))\n",
    "\n",
    "# api data\n",
    "split_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices SPLIT/')\n",
    "split_fsyms = [i.split('.')[0] for i in split_fsyms]\n",
    "print(len(split_fsyms))\n",
    "\n",
    "# combined equity fysm universe\n",
    "equity_fsyms = list(set(excel_fsyms) | set(split_fsyms))\n",
    "print('total fsyms', len(equity_fsyms))\n",
    "\n",
    "completed_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/processed/')\n",
    "completed_fsyms = [i.split('.')[0] for i in completed_fsyms]\n",
    "\n",
    "equity_fsyms = [f for f in equity_fsyms if not f in completed_fsyms]\n",
    "print('--remaining', len(equity_fsyms))\n",
    "\n",
    "# build a dictionary with mappings from exchange country to region\n",
    "temp = pd.read_csv('/Users/joeybortfeld/Documents/QML Solutions Data/universe_and_traits/country_to_region_mapping.csv')\n",
    "country_to_region_dict = temp.set_index('exchange_country')['region'].to_dict()\n",
    "\n",
    "# build a dictionary with mappings from fsym_id to exchange country\n",
    "temp = pd.read_csv('/Users/joeybortfeld/Documents/QML Solutions Data/universe_and_traits/qml_universe_ids.csv')\n",
    "temp['region'] = temp['exchange_country'].map(country_to_region_dict)\n",
    "fsym_to_country_dict = temp.set_index('fsym_id')['exchange_country'].to_dict()\n",
    "\n",
    "region_to_benchmark_dict = {\n",
    "    'North America': 'SP500',\n",
    "    'Europe': 'STOXX',\n",
    "    'Japan': 'NIKKEI',\n",
    "    'Asia ex-Japan': 'None',\n",
    "    'Africa': 'None', \n",
    "    'Middle East': 'None',\n",
    "    'South America': 'None',\n",
    "    '@NA': 'None',\n",
    "    np.nan: 'None'\n",
    "}\n",
    "\n",
    "collection = []\n",
    "for this_fsym in tqdm.tqdm(equity_fsyms):\n",
    "\n",
    "    temp = qml_equity_ratios.merge_equity_data(fsym_id=this_fsym)\n",
    "    temp['fsym_id'] = this_fsym\n",
    "    this_country = fsym_to_country_dict[this_fsym]\n",
    "    this_region = country_to_region_dict[this_country]\n",
    "    this_benchmark = region_to_benchmark_dict[this_region]\n",
    "    \n",
    "    # skip if fsym_id is from a region that we do not map to a benchmark\n",
    "    if this_benchmark == 'None':\n",
    "        continue\n",
    "\n",
    "    temp = qml_equity_ratios.combine_benchmark_data(temp, df_benchmarks, benchmark=this_benchmark)\n",
    "\n",
    "    temp1a = qml_equity_ratios.calc_capm(temp, trailing_periods_list=[182,365], frequency='ME', outlier_drops=4, downside_only=False, exponential_weighting=(False, 0.99))\n",
    "    temp1b = qml_equity_ratios.calc_capm(temp, trailing_periods_list=[182,365], frequency='ME', outlier_drops=4, downside_only=True, exponential_weighting=(False, 0.99))\n",
    "    temp2 = qml_equity_ratios.calc_rolling_returns(temp)\n",
    "    temp3 = qml_equity_ratios.calc_drawdown(temp)\n",
    "    temp4 = qml_equity_ratios.calc_downside_volatility(temp)\n",
    "    temp5 = qml_equity_ratios.calc_ulcer_index(temp)\n",
    "\n",
    "    df = temp[['fsym_id', 'date', 'market_cap', 'price', 'volume', ]].merge(temp1a, on=['date'], how='outer')\n",
    "    df = df.merge(temp1b, on=['date'], how='outer')\n",
    "    df = df.merge(temp2, on=['date',], how='outer')\n",
    "    df = df.merge(temp3, on=['date',], how='outer')\n",
    "    df = df.merge(temp4, on=['date',], how='outer')\n",
    "    df = df.merge(temp5, on=['date',], how='outer')\n",
    "\n",
    "    # fillin missing \n",
    "    # - for the monthly calculations (CAPM, etc) we assume exact month end dates (10/31)\n",
    "    # - but for the daily data (price, market cap, returns, etc) the last trading date may not be true month end (10/29)\n",
    "    for c in ['market_cap', 'price', 'volume', 'return_1', 'return_2', 'return_3', 'return_6', 'return_12']:\n",
    "        df[c] = df[c].ffill(limit=10)\n",
    "    df['fsym_id'] = this_fsym\n",
    "\n",
    "    # drop obs out of bounds of the available data date range\n",
    "    min_date = df[df['price'].notnull()]['date'].min()\n",
    "    max_date = df[df['price'].notnull()]['date'].max()\n",
    "\n",
    "    # reduce to monthly\n",
    "    df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "    df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "    df['max_date'] = df.groupby(['year', 'month'])['date'].transform('max')\n",
    "    df = df[df['date'] == df['max_date']]\n",
    "    df = df.drop(columns=['year', 'month', 'max_date'])\n",
    "\n",
    "    df = df[df['date'] >= min_date]\n",
    "    df = df[df['date'] <= max_date]\n",
    "\n",
    "    df.to_csv(f'/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/processed/{this_fsym}.csv', index=False)\n",
    "    collection.append(df)\n",
    "\n",
    "collection = pd.concat(collection, axis=0)\n",
    "collection.to_csv('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_consolidated/equity_ratios_combined.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[['date', 'price', 'benchmark']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection['fsym_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_fsyms[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'liquidity'\n",
    "# metric = 'P-value'\n",
    "# metric = 'Coefficient'\n",
    "metric ='AUROC - Train'\n",
    "\n",
    "collection = []\n",
    "for i in [1,2,3,4,5]:\n",
    "    temp = pd.read_csv(f'/Users/joeybortfeld/Downloads/univariate_reg_{i}y-pct_split.csv')\n",
    "    temp['t'] = i\n",
    "    collection.append(temp)\n",
    "\n",
    "df = pd.concat(collection, axis=0)\n",
    "\n",
    "print(df['Category'].unique())\n",
    "\n",
    "df['P-value'] = df['P-value'].map(lambda x: f\"{x:.3f}\")\n",
    "df['AUROC - Train'] = df['AUROC - Train'].map(lambda x: f\"{x:.2f}\")\n",
    "df['Coefficient'] = df['Coefficient'].map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "\n",
    "df = df[df['Category'] == category]\n",
    "\n",
    "df = df.pivot(index='Variable', columns='t', values=metric)\n",
    "df = df.sort_values(by=5, ascending=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "def merton_distance_to_default(market_cap, debt, equity_vol, risk_free_rate=0.03, time_horizon=1):\n",
    "    \"\"\" Computes Distance to Default (DD) using Merton's structural model. \"\"\"\n",
    "    \n",
    "    # Initial guess: Assume asset value is close to market cap\n",
    "    asset_value = market_cap\n",
    "    asset_vol = equity_vol  # Approximate initial asset volatility\n",
    "\n",
    "    def equations(vars):\n",
    "        A, sigma_A = vars\n",
    "        d1 = (np.log(A / debt) + (risk_free_rate + 0.5 * sigma_A ** 2) * time_horizon) / (sigma_A * np.sqrt(time_horizon))\n",
    "        d2 = d1 - sigma_A * np.sqrt(time_horizon)\n",
    "\n",
    "        eq1 = market_cap - (A * norm.cdf(d1) - np.exp(-risk_free_rate * time_horizon) * debt * norm.cdf(d2))\n",
    "        eq2 = equity_vol * market_cap - norm.cdf(d1) * A * sigma_A\n",
    "\n",
    "        return [eq1, eq2]\n",
    "\n",
    "    # Solve for asset value (A) and asset volatility (sigma_A)\n",
    "    A, sigma_A = fsolve(equations, [asset_value, asset_vol])\n",
    "\n",
    "    # Compute Distance to Default\n",
    "    d1 = (np.log(A / debt) + (risk_free_rate + 0.5 * sigma_A ** 2) * time_horizon) / (sigma_A * np.sqrt(time_horizon))\n",
    "    d2 = d1 - sigma_A * np.sqrt(time_horizon)\n",
    "    \n",
    "    distance_to_default = d2\n",
    "    probability_of_default = norm.cdf(-distance_to_default)\n",
    "\n",
    "    return distance_to_default, probability_of_default\n",
    "\n",
    "# Example Firm Data\n",
    "market_cap = 36_702_000_000  # $5 billion\n",
    "debt = 3_000_000_000        # $3 billion in debt\n",
    "equity_vol = 0.30           # 30% annualized volatility\n",
    "\n",
    "# Compute DD and PD\n",
    "dd, pd = merton_distance_to_default(market_cap, debt, equity_vol)\n",
    "print(f\"Distance to Default: {dd:.7f}\")\n",
    "print(f\"Probability of Default: {pd:.7%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp['date'] < pd.to_datetime('2000-01-01')].set_index('date')['price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices SPLIT/')\n",
    "split_fsyms = [i.split('.')[0] for i in split_fsyms]\n",
    "\n",
    "unsplit_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices UNSPLIT/')\n",
    "unsplit_fsyms = [i.split('.')[0] for i in unsplit_fsyms]\n",
    "\n",
    "excel_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/excel_addin_download/')\n",
    "excel_fsyms = [i.split('_')[0] for i in excel_fsyms]\n",
    "\n",
    "share_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/shares/')\n",
    "share_fsyms = [i.split('.')[0] for i in share_fsyms]\n",
    "\n",
    "print('split', len(split_fsyms))\n",
    "print('unsplit', len(unsplit_fsyms))\n",
    "print('excel', len(excel_fsyms))\n",
    "print('shares', len(share_fsyms))\n",
    "\n",
    "[f for f in excel_fsyms if not f in split_fsyms]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, temp1 = factset_api.get_stock_prices(id_list=[this_fsym], \n",
    "field_list=['price', 'volume', 'tradeCount'], \n",
    "start_date='2006-01-06', \n",
    "end_date='2024-12-31', \n",
    "frequency='D',\n",
    "split='SPLIT',\n",
    "verbose=True,\n",
    "authorization=credentials.factset_api_authorization)\n",
    "\n",
    "temp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fsym = 'MH33D6-R'\n",
    "\n",
    "response, temp1 = factset_api.get_stock_prices(id_list=[this_fsym], \n",
    "field_list=['price', 'volume', 'tradeCount'], \n",
    "start_date='2006-01-06', \n",
    "end_date='2024-12-31', \n",
    "frequency='D',\n",
    "split='SPLIT',\n",
    "authorization=credentials.factset_api_authorization)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "temp.set_index('date')['price'].plot(ax=axes[0])\n",
    "\n",
    "response, temp2 = factset_api.get_shares_outanding(id_list=[this_fsym], \n",
    "                     start_date='2006-03-31', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='M',\n",
    "                     verbose=False,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "\n",
    "print(response)\n",
    "temp2.set_index('date')['totalOutstanding'].plot(ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp1.merge(temp2, on='date', how='outer')\n",
    "temp['totalOutstanding'] = temp['totalOutstanding'].fillna(method='ffill')\n",
    "fig, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "temp.set_index('date')[['totalOutstanding', 'price']].plot(secondary_y='totalOutstanding', ax=ax[0])\n",
    "\n",
    "temp['market_cap'] = temp['totalOutstanding'] * temp['price']\n",
    "temp['market_cap'].plot(ax=ax[1])\n",
    "ax[0].set_title('Total Outstanding and Price')\n",
    "ax[1].set_title('Market Cap')\n",
    "plt.show()\n",
    "\n",
    "temp['year'] = pd.to_datetime(temp['date']).dt.year\n",
    "temp = temp.drop_duplicates(subset=['year'], keep='last')\n",
    "temp['market_cap'] /= 1_000\n",
    "temp[['date', 'market_cap', 'price']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Download using SPLIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, temp = factset_api.download_fundamentals(id_list=['MH33D6-R'], \n",
    "                                #   field_list=['FF_IS_MULTI_SHARE', 'FF_IS_ADR'],\n",
    "                                  field_list=['FF_COM_SHS_OUT'],\n",
    "                                    periodicity='ANN', \n",
    "                                    start_date='1990-01-01', \n",
    "                                    end_date='2024-12-31', \n",
    "                                    currency='LOCAL',\n",
    "                                    update_type='RP', \n",
    "                                    verbose=True, \n",
    "                                    authorization=credentials.factset_api_authorization)\n",
    "temp.set_index('reportDate')['value'].plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://api.factset.com/content/factset-global-prices/v1/prices?ids=CGF31Z-R&fields=price,priceOpen,priceHigh,priceLow,volume&startDate=2024-01-01&endDate=2024-08-27&frequency=D&calendar=FIVEDAY&currency=EUR&adjust=SPLIT&batch=N'\n",
    "\n",
    "headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "response = requests.get(url, headers=headers, auth = credentials.factset_api_authorization)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cagr(initial, final, years):\n",
    "    return (final / initial) ** (1 / years) - 1\n",
    "\n",
    "# Parameters\n",
    "initial_revenue = 1.00\n",
    "initial_expenses = 0.30\n",
    "initial_net_income = initial_revenue - initial_expenses\n",
    "\n",
    "years = 5\n",
    "\n",
    "# Define revenue and expense growth rates to iterate over\n",
    "revenue_growth_rates = np.arange(0.01, 0.1, 0.01)  # 1% to 5%\n",
    "expense_growth_rates = np.arange(0.01, 0.1, 0.01)  # 1% to 5%\n",
    "\n",
    "# Create table\n",
    "cagr_table = pd.DataFrame(index=[f\"{e*100:.0f}%\" for e in expense_growth_rates],\n",
    "                          columns=[f\"{r*100:.0f}%\" for r in revenue_growth_rates])\n",
    "\n",
    "for e_growth in expense_growth_rates:\n",
    "    for r_growth in revenue_growth_rates:\n",
    "        # Compute revenue and expense projections\n",
    "        final_revenue = initial_revenue * (1 + r_growth) ** years\n",
    "        final_expenses = initial_expenses * (1 + e_growth) ** years\n",
    "        final_net_income = final_revenue - final_expenses\n",
    "        \n",
    "        # Compute CAGR of net income\n",
    "        cagr_net_income = calculate_cagr(initial_net_income, final_net_income, years)\n",
    "        cagr_table.loc[f\"{e_growth*100:.0f}%\", f\"{r_growth*100:.0f}%\"] = f\"{cagr_net_income*100:.2f}%\"\n",
    "\n",
    "# Display the result\n",
    "cagr_table.to_csv('/Users/joeybortfeld/Downloads/cagr_table.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
