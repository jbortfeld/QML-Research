{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import tqdm\n",
    "import os\n",
    "import boto3\n",
    "from Py_Files import credentials\n",
    "from Py_Files import factset_api\n",
    "from Py_Files import factset_fields\n",
    "from Py_Files import qml_ratios\n",
    "from Py_Files import qml_equity_ratios\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = '/Users/joeybortfeld/Documents/QML Solutions Data/'\n",
    "s3_dir = 's3://qml-research-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factset_universe = pd.read_csv(data_dir + '/universe_and_traits/qml_universe_ids.csv')\n",
    "universe_dict = factset_api.load_universe_dict(factset_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the universe to use\n",
    "selected_universe = 'us_nonfin_100m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Consolidate all fundamentaldata into a single dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_from_source_files = True\n",
    "\n",
    "if build_from_source_files: \n",
    "\n",
    "    print('building from source files')\n",
    "\n",
    "    \n",
    "    fsym_list = universe_dict[selected_universe]\n",
    "\n",
    "    df_annual, error_list_annual = qml_ratios.consolidate_selected_files(fsym_list=fsym_list, folder_path=data_dir + 'factset_data/factset_fundamentals/annual/')\n",
    "    df_annual = qml_ratios.preprocess_factset_fundamentals(df_annual, verbose=True) \n",
    "    df_annual.to_csv(data_dir + f'factset_data/factset_consolidated/annual_fundamentals_combined_{selected_universe}.csv', index=False)\n",
    "\n",
    "    df_quarterly, error_list_quarterly = qml_ratios.consolidate_selected_files(fsym_list=fsym_list, folder_path=data_dir + 'factset_data/factset_fundamentals/quarterly/')\n",
    "    df_quarterly = qml_ratios.preprocess_factset_fundamentals(df_quarterly, verbose=True)\n",
    "    df_quarterly.to_csv(data_dir + f'factset_data/factset_consolidated/quarterly_fundamentals_combined_{selected_universe}.csv', index=False)\n",
    "\n",
    "    df_semi_annual, error_list_semi_annual = qml_ratios.consolidate_selected_files(fsym_list=fsym_list, folder_path=data_dir + 'factset_data/factset_fundamentals/semi_annual/')\n",
    "    df_semi_annual = qml_ratios.preprocess_factset_fundamentals(df_semi_annual, verbose=True)\n",
    "    df_semi_annual.to_csv(data_dir + f'factset_data/factset_consolidated/semi_annual_fundamentals_combined_{selected_universe}.csv', index=False)\n",
    "\n",
    "    # check for any columns that are not in the flow or stock variable lists\n",
    "    temp = [c for c in df_annual.columns if c not in factset_fields.flow_var_list + factset_fields.stock_var_list]    \n",
    "    print('data validation:')\n",
    "    print('unexpected columns:', temp)\n",
    "    print()\n",
    "\n",
    "    # COLLECT ASSETS IN USD DATA\n",
    "\n",
    "    df_annual_assets_in_usd, error_list_annual = qml_ratios.consolidate_selected_files(fsym_list=fsym_list, folder_path=data_dir + 'factset_data/factset_assets_in_usd/annual/')\n",
    "    df_annual_assets_in_usd = qml_ratios.preprocess_factset_fundamentals(df_annual_assets_in_usd, verbose=True) \n",
    "    df_annual_assets_in_usd.to_csv(data_dir + f'factset_data/factset_consolidated/annual_assets_in_usd_{selected_universe}.csv', index=False)\n",
    "\n",
    "    df_semi_annual_assets_in_usd, error_list_semi_annual = qml_ratios.consolidate_selected_files(fsym_list=fsym_list, folder_path=data_dir + 'factset_data/factset_assets_in_usd/semi_annual/')\n",
    "    df_semi_annual_assets_in_usd = qml_ratios.preprocess_factset_fundamentals(df_semi_annual_assets_in_usd, verbose=True) \n",
    "    df_semi_annual_assets_in_usd.to_csv(data_dir + f'factset_data/factset_consolidated/semi_annual_assets_in_usd_{selected_universe}.csv', index=False)\n",
    "\n",
    "    df_quarterly_assets_in_usd, error_list_quarterly = qml_ratios.consolidate_selected_files(fsym_list=fsym_list, folder_path=data_dir + 'factset_data/factset_assets_in_usd/quarterly/')\n",
    "    df_quarterly_assets_in_usd = qml_ratios.preprocess_factset_fundamentals(df_quarterly_assets_in_usd, verbose=True) \n",
    "    df_quarterly_assets_in_usd.to_csv(data_dir + f'factset_data/factset_consolidated/quarterly_assets_in_usd_{selected_universe}.csv', index=False)\n",
    "\n",
    "else: \n",
    "\n",
    "    df_annual = pd.read_csv(data_dir + f'factset_data/factset_consolidated/annual_fundamentals_combined_{selected_universe}.csv')\n",
    "    df_quarterly = pd.read_csv(data_dir + f'factset_data/factset_consolidated/quarterly_fundamentals_combined_{selected_universe}.csv')\n",
    "    df_semi_annual = pd.read_csv(data_dir + f'factset_data/factset_consolidated/semi_annual_fundamentals_combined_{selected_universe}.csv')\n",
    "    df_annual_assets_in_usd = pd.read_csv(data_dir + f'factset_data/factset_consolidated/annual_assets_in_usd_{selected_universe}.csv')\n",
    "    df_semi_annual_assets_in_usd = pd.read_csv(data_dir + f'factset_data/factset_consolidated/semi_annual_assets_in_usd_{selected_universe}.csv')\n",
    "    df_quarterly_assets_in_usd = pd.read_csv(data_dir + f'factset_data/factset_consolidated/quarterly_assets_in_usd_{selected_universe}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the fundamental data\n",
    "# - get quarterly, semi-annual, annual data\n",
    "# - combine into a single dataframe\n",
    "# - construct ratios\n",
    "\n",
    "build_from_source_files = True\n",
    "if build_from_source_files:\n",
    "    df_annual_formatted = qml_ratios.format_annual_data(df_annual, \n",
    "                                         flow_vars=factset_fields.flow_var_list, \n",
    "                                         stock_vars=factset_fields.stock_var_list, \n",
    "                                         verbose=True)\n",
    "\n",
    "    df_quarterly_formatted = qml_ratios.format_quarterly_data(df_quarterly, \n",
    "                                                flow_vars=factset_fields.flow_var_list, \n",
    "                                                stock_vars=factset_fields.stock_var_list, \n",
    "                                                verbose=True) \n",
    "\n",
    "    df_semi_annual_formatted = qml_ratios.format_semi_annual_data(df_semi_annual, \n",
    "                                                flow_vars=factset_fields.flow_var_list, \n",
    "                                                stock_vars=factset_fields.stock_var_list, \n",
    "                                                verbose=True) \n",
    "\n",
    "    df_merged = qml_ratios.merge_quarterly_semi_and_annual(quarterly=df_quarterly_formatted, \n",
    "                                                semi_annual=df_semi_annual_formatted, \n",
    "                                                annual=df_annual_formatted, \n",
    "                                                flow_vars=factset_fields.flow_var_list, \n",
    "                                                stock_vars=factset_fields.stock_var_list, \n",
    "                                                cleanup=True)\n",
    "\n",
    "\n",
    "    df_assets_in_usd_formatted = qml_ratios.format_assets_in_usd_data(data_annual=df_annual_assets_in_usd, \n",
    "                                                                      data_semi_annual=df_semi_annual_assets_in_usd, \n",
    "    data_quarterly=df_quarterly_assets_in_usd, \n",
    "    cleanup=True)\n",
    "    df_merged = df_merged.merge(df_assets_in_usd_formatted, on=['fsym_id', 'fiscal_end_date'], how='left')\n",
    "\n",
    "    # construct ratios\n",
    "    df = qml_ratios.build_qml_model_ratios(df_merged, verbose=True)\n",
    "\n",
    "    earnings_volatility_qf = qml_ratios.calculate_earnings_volatility(df_quarterly_formatted, freq='qf')\n",
    "    earnings_volatility_saf = qml_ratios.calculate_earnings_volatility(df_semi_annual_formatted, freq='saf')\n",
    "    df = df.merge(earnings_volatility_qf, on=['fsym_id', 'fiscal_end_date'], how='left')\n",
    "    df = df.merge(earnings_volatility_saf, on=['fsym_id', 'fiscal_end_date'], how='left')\n",
    "    for var in ['net_income_vol', 'ebitda_vol', 'ebit_vol', 'sales_vol']:\n",
    "        df[var] = df[f'{var}_qf'].fillna(df[f'{var}_saf'])\n",
    "    print('done')\n",
    "\n",
    "    todays_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "    df.to_csv(data_dir + f'qml_modeling_data/fundamental_dataset_{selected_universe}_{todays_date}.csv', index=False)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(data_dir + f'qml_modeling_data/fundamental_dataset_20250115.csv')\n",
    "    df['fiscal_end_date'] = pd.to_datetime(df['fiscal_end_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Consolidate equity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect equity benchmark data\n",
    "df_benchmarks = qml_equity_ratios.build_benchmark_data()\n",
    "\n",
    "# build a set of dictionaries that help us map from fsym_id to benchmark index\n",
    "fsym_to_country_dict = qml_equity_ratios.build_fsym_to_country_dict()\n",
    "country_to_region_dict = qml_equity_ratios.build_country_to_region_dict()\n",
    "region_to_benchmark_dict = qml_equity_ratios.build_region_to_benchmark_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the fsyms and calculate the equity ratios. write a csv for each fsym with combined equity data/ratios\n",
    "build_from_source_files = False\n",
    "\n",
    "if build_from_source_files:\n",
    "\n",
    "    excel_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/excel_addin_download/')\n",
    "    excel_fsyms = [i.split('_')[0] for i in excel_fsyms]\n",
    "    print(len(excel_fsyms))\n",
    "\n",
    "    # api data\n",
    "    split_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices SPLIT/')\n",
    "    split_fsyms = [i.split('.')[0] for i in split_fsyms]\n",
    "    print(len(split_fsyms))\n",
    "\n",
    "    # combined equity fysm universe\n",
    "    equity_fsyms = list(set(excel_fsyms) | set(split_fsyms))\n",
    "    print('total fsyms', len(equity_fsyms))\n",
    "\n",
    "    # filter out fsyms that have already been processed\n",
    "    completed_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/processed/')\n",
    "    completed_fsyms = [i.split('.')[0] for i in completed_fsyms]\n",
    "\n",
    "    equity_fsyms = [f for f in equity_fsyms if not f in completed_fsyms]\n",
    "    print('--remaining', len(equity_fsyms))\n",
    "\n",
    "    # calculate equity ratios\n",
    "    status = qml_equity_ratios.build_equity_ratios(equity_fsyms, \n",
    "                                        df_benchmarks, \n",
    "                                        fsym_to_country_dict, \n",
    "                                        country_to_region_dict, \n",
    "                                        region_to_benchmark_dict, \n",
    "                                        output_dir='/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccollect equity data for all fsyms and consolidate results\n",
    "\n",
    "build_from_source_files = True\n",
    "if build_from_source_files:\n",
    "\n",
    "    fsym_list = universe_dict[selected_universe]\n",
    "\n",
    "    completed_fsyms = os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/processed/')\n",
    "    completed_fsyms = [i.split('.')[0] for i in completed_fsyms]\n",
    "    completed_fsyms.remove('')\n",
    "\n",
    "    fsym_list = [f for f in fsym_list if f in completed_fsyms]\n",
    "\n",
    "    collection = []\n",
    "    for f in tqdm.tqdm(completed_fsyms):\n",
    "        temp = pd.read_csv(data_dir + f'factset_data/factset_equity/processed/{f}.csv')\n",
    "        collection.append(temp)\n",
    "\n",
    "    df_equity_ratios = pd.concat(collection, axis=0)\n",
    "    df_equity_ratios.to_csv(data_dir + f'factset_data/factset_consolidated/equity_ratios_combined_{selected_universe}.csv', index=False)\n",
    "\n",
    "else:\n",
    "    df_equity_ratios = pd.read_csv(data_dir + f'factset_data/factset_consolidated/equity_ratios_combined_{selected_universe}.csv')\n",
    "    df_equity_ratios['date'] = pd.to_datetime(df_equity_ratios['date'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Combine fundamental data/ratios with equity data/ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine fundamental data/ratios with equity data/ratios\n",
    "\n",
    "####################################################################\n",
    "# fundamental data\n",
    "df = pd.read_csv(data_dir + f'qml_modeling_data/fundamental_dataset_us_nonfin_100m_20250212.csv')\n",
    "\n",
    "for c in ['fiscal_end_date', 'eps_report_date_qf', 'eps_report_date_saf', 'eps_report_date_af']:\n",
    "    df[c] = pd.to_datetime(df[c])\n",
    "\n",
    "# construct report date\n",
    "df['report_date'] = df['eps_report_date_qf'].fillna(df['eps_report_date_saf']).fillna(df['eps_report_date_af'])\n",
    "\n",
    "# if no report date is available, use the fiscal date and offset by 90 days\n",
    "mask = df['report_date'].isnull()\n",
    "df.loc[mask, 'report_date'] = df.loc[mask, 'fiscal_end_date'].map(lambda x: x + pd.Timedelta(days=90))\n",
    "\n",
    "# reset the report date to the last day of the month\n",
    "df['report_date'] = df['report_date'] + pd.offsets.MonthEnd(0)\n",
    "df['report_month'] = df['report_date'].dt.month\n",
    "df['report_year'] = df['report_date'].dt.year\n",
    "\n",
    "####################################################################\n",
    "# convert from quarterly/semi-annual/annual observations to monthly observations\n",
    "temp = df[['fsym_id', 'report_date']].copy()\n",
    "temp['max_report_date'] = temp.groupby(by='fsym_id')['report_date'].transform('max')\n",
    "temp['min_report_date'] = temp.groupby(by='fsym_id')['report_date'].transform('min')\n",
    "temp = temp.drop_duplicates(subset='fsym_id', keep='first')\n",
    "temp = temp[['fsym_id', 'min_report_date', 'max_report_date']]\n",
    "\n",
    "print('build monthly template')\n",
    "all_fsyms = temp['fsym_id'].unique()\n",
    "all_dates = pd.date_range(start=df['report_date'].min(), end=df['report_date'].max(), freq='ME')\n",
    "fsym_date_combos = []\n",
    "for f in tqdm.tqdm(all_fsyms):\n",
    "    for d in all_dates:\n",
    "        fsym_date_combos.append((f, d))\n",
    "fsym_date_df = pd.DataFrame(fsym_date_combos, columns=['fsym_id', 'report_date'])\n",
    "\n",
    "fsym_date_df = fsym_date_df.merge(temp, on='fsym_id', how='left')\n",
    "\n",
    "# trim the template to the range of report dates for each fsym\n",
    "print('--pre-trim:', fsym_date_df.shape)\n",
    "fsym_date_df = fsym_date_df[fsym_date_df['report_date'] >= fsym_date_df['min_report_date']]\n",
    "fsym_date_df = fsym_date_df[fsym_date_df['report_date'] <= fsym_date_df['max_report_date']]\n",
    "print('--post-trim:', fsym_date_df.shape)\n",
    "\n",
    "df = fsym_date_df.merge(df, on=['fsym_id', 'report_date'], how='outer')\n",
    "df = df.sort_values(by=['fsym_id', 'report_date'])\n",
    "\n",
    "# fill forward the fundamental data into the monthly template\n",
    "fund_cols = [c for c in df.columns]\n",
    "print('fill forward fundamentals into monthly template')\n",
    "for c in tqdm.tqdm(fund_cols):\n",
    "    df[c] = df.groupby('fsym_id')[c].ffill(limit=14)\n",
    "\n",
    "# recalc report year and month in case of gaps from forward fill\n",
    "df['report_month'] = df['report_date'].dt.month\n",
    "df['report_year'] = df['report_date'].dt.year\n",
    "\n",
    "####################################################################\n",
    "# merge equity data\n",
    "print('merge equity data')\n",
    "df_equity_ratios['date'] = pd.to_datetime(df_equity_ratios['date'])\n",
    "df_equity_ratios['report_month'] = df_equity_ratios['date'].dt.month\n",
    "df_equity_ratios['report_year'] = df_equity_ratios['date'].dt.year\n",
    "\n",
    "df = df.merge(df_equity_ratios, on=['fsym_id', 'report_month', 'report_year'], how='outer')\n",
    "df = df.sort_values(by=['fsym_id', 'report_year', 'report_month'])\n",
    "\n",
    "########################################################\n",
    "# market leverage - the only combination of fundamental and equity data\n",
    "# market cap is in millions, ff_liabs is in millions\n",
    "df['market_leverage'] = df['ff_liabs'] / (df['market_cap'] + df['ff_liabs']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Descriptive Data and Bankruptcy Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. get coverage data\n",
    "df_coverage = pd.read_csv(data_dir + 'universe_and_traits/qml_universe_ids.csv')\n",
    "df_coverage = df_coverage[['fsym_id', 'name1', 'name2', 'factset_econ_sector', 'factset_industry', \n",
    "                       'entity_country_hq', 'exchange_country',\n",
    "                       'max_assets_in_usd', 'factset_entity_id', 'ultimate_parent_id']]\n",
    "df_coverage = df_coverage[df_coverage['fsym_id'] != '@NA']\n",
    "\n",
    "print('coverage data shape:', df_coverage.shape)\n",
    "print(df_coverage['factset_econ_sector'].value_counts())\n",
    "print()\n",
    "\n",
    "# merge company descriptive data\n",
    "df = df.merge(df_coverage, on='fsym_id', how='left')\n",
    "\n",
    "# 2. get company default data\n",
    "df_defaults = pd.read_csv(data_dir + 'universe_and_traits/bankruptcy_data.csv')\n",
    "df_defaults['bankruptcy_date'] = pd.to_datetime(df_defaults['bankruptcy_date'])\n",
    "df_defaults = df_defaults[['fsym_id', 'bankruptcy_date']]\n",
    "df_defaults = df_defaults[df_defaults['bankruptcy_date'].notnull()]\n",
    "df_defaults = df_defaults[df_defaults['fsym_id'] != '@NA']\n",
    "df_defaults = df_defaults[df_defaults['fsym_id'] != '']\n",
    "df_defaults = df_defaults[df_defaults['fsym_id'].notnull()]\n",
    "validation = df_defaults.duplicated(subset='fsym_id', keep='first').sum()\n",
    "if validation > 0:\n",
    "    print('ALERT: bankruptcy duplicates found')\n",
    "    print('bankruptcy duplicates:', validation)\n",
    "df_defaults = df_defaults.sort_values(by=['fsym_id', 'bankruptcy_date'], ascending=False)\n",
    "df_defaults = df_defaults.drop_duplicates(subset='fsym_id', keep='last')\n",
    "\n",
    "df = df.merge(df_defaults, on='fsym_id', how='left')\n",
    "\n",
    "# 3. drop financial companies (banks, insurance, finance)\n",
    "mask1 = df['factset_econ_sector'] == 'Finance'\n",
    "mask2 = df['factset_industry'] != 'Real Estate Development'\n",
    "df = df[~(mask1 & mask2)]\n",
    "\n",
    "print('fsym_ids with bankruptcy:', df[df['bankruptcy_date'].notnull()]['fsym_id'].nunique())\n",
    "\n",
    "# label forward defaults over 1,2,3,4,5 years\n",
    "print('labeling defaults')\n",
    "for i in tqdm.tqdm([1,2,3,4,5]):\n",
    "\n",
    "    df[f'default_{i}'] = 0\n",
    "    mask1 = (df['bankruptcy_date'] - df['report_date']).dt.days < (365*i + 365*0.5)\n",
    "    mask2 = (df['bankruptcy_date'] - df['report_date']).dt.days >= (365*i - 365*0.5)\n",
    "    df.loc[mask1 & mask2, f'default_{i}'] = 1\n",
    "\n",
    "    # flag -1 defaults\n",
    "    mask1 = (df['bankruptcy_date'] - df['report_date']).dt.days < (365*i - 365*0.5)\n",
    "    df.loc[mask1, f'default_{i}'] = -1\n",
    "\n",
    "print('write to csv')\n",
    "todays_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "df.to_csv(data_dir + f'qml_modeling_data/modeling_dataset_with_bankruptcy_labels_{selected_universe}_{todays_date}.csv', index=False)\n",
    "print('done all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple diagnostics\n",
    "print('fsym count:',df['fsym_id'].nunique())\n",
    "print('default rate:', df[df['default_1'] != -1]['default_1'].mean())\n",
    "print('defaulted fsym count:', df[df['default_1'] == 1]['fsym_id'].nunique())\n",
    "print('total obs count:', df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company-specific diagnostics\n",
    "temp = df[df['fsym_id'] == 'MH33D6-R'].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12), ncols=3, nrows=3)\n",
    "temp.set_index('date')['ff_assets_in_usd'].plot(ax=ax[0, 0], title='ff_assets_in_usd')\n",
    "temp.set_index('fiscal_end_date')['net_income_to_sales'].plot(ax=ax[0, 1], title='net_income_to_sales')\n",
    "temp.set_index('fiscal_end_date')['ebitda_to_interest_expense'].plot(ax=ax[0, 2], title='ebitda_to_interest_expense')\n",
    "\n",
    "temp.set_index('date')['capm_idio_vol_365'].plot(ax=ax[1, 0], title='capm_idio_vol_365')\n",
    "temp.set_index('date')['return_12'].plot(ax=ax[1, 1], title='return_12')\n",
    "temp.set_index('date')['ulcer_index_128'].plot(ax=ax[1, 2], title='ulcer_index_182')\n",
    "\n",
    "temp.set_index('date')['price'].plot(ax=ax[2, 0], title='price')\n",
    "temp.set_index('date')['market_cap'].plot(ax=ax[2, 1], title='market_cap')\n",
    "temp.set_index('date')['market_leverage'].plot(ax=ax[2, 2], title='market_leverage')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "temp[['fsym_id', 'fiscal_end_date', 'date', 'net_income_to_sales', 'capm_idio_vol_365']].tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
