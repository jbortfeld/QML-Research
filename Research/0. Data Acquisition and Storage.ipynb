{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import tqdm\n",
    "import os\n",
    "import boto3\n",
    "from Py_Files import credentials\n",
    "from Py_Files import factset_api\n",
    "from Py_Files import factset_fields\n",
    "\n",
    "data_dir = '/Users/joeybortfeld/Documents/QML Solutions Data/'\n",
    "s3_dir = 's3://qml-research-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load the Factset Universe (All Fsym IDS) into Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factset_universe = pd.read_csv('/Users/joeybortfeld/Downloads/qml_universe_ids.csv')\n",
    "\n",
    "universe_dict = factset_api.load_universe_dict(factset_universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download Assets in USD using the Factset Fundamentals API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = factset_api.batch_fundamental_download(fsym_list=universe_dict['$1B'],\n",
    "                               field_list=['FF_ASSETS'],\n",
    "                               currency='USD',\n",
    "                               periodicity_list=['annual', 'quarterly' 'semi_annual'],\n",
    "                               start_date='1990-01-01',\n",
    "                               end_date='2024-12-31',\n",
    "                               skip_if_done=True,\n",
    "                               output_folder='/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_assets_in_usd/',\n",
    "                               factset_api_authorization=credentials.factset_api_authorization)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download All Metrics in Local Currency using the Factset Fundamentals API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = factset_api.batch_fundamental_download(fsym_list=universe_dict['full'],\n",
    "                               field_list=factset_fields.fundamental_fields,\n",
    "                               currency='LOCAL',\n",
    "                               periodicity_list=['annual', 'quarterly' 'semi_annual'],\n",
    "                               start_date='1990-01-01',\n",
    "                               end_date='2024-12-31',\n",
    "                               skip_if_done=True,\n",
    "                               output_folder='/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/',\n",
    "                               factset_api_authorization=credentials.factset_api_authorization)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Review Downloaded Data on Local Storage and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the file counts stored locally\n",
    "\n",
    "folder_list = [\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/annual/',\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/quarterly/',\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/semi_annual/',\n",
    "\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_assets_in_usd/annual/',\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_assets_in_usd/semi_annual/',\n",
    "]\n",
    "\n",
    "for this_folder in folder_list:\n",
    "    file_list = os.listdir(this_folder)\n",
    "    \n",
    "    print(this_folder, len(file_list))\n",
    "\n",
    "        # for this_file in file_list:\n",
    "    #     aws_s3.copy_file_to_s3(local_file_path=this_folder + this_file, \n",
    "    #                             s3_bucket='qml-solutions-new-york', \n",
    "    #                             s3_key='factset-api-fundamentals/', \n",
    "    #                             aws_access_key_id=credentials.aws_access_key_id, \n",
    "    #                             aws_secret_access_key=credentials.aws_secret_access_key,\n",
    "    #                             verbose=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer local files to s3\n",
    "\n",
    "\n",
    "folder_list = [\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/annual/',\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/quarterly/',\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_fundamentals/semi_annual/',\n",
    "\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_assets_in_usd/annual/',\n",
    "    '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_assets_in_usd/semi_annual/',\n",
    "]\n",
    "\n",
    "for this_folder in folder_list:\n",
    "    file_list = os.listdir(this_folder)\n",
    "    \n",
    "    print(this_folder, len(file_list))\n",
    "\n",
    "    for this_file in tqdm.tqdm(file_list):\n",
    "        aws_s3.copy_file_to_s3(local_file_path=this_folder + this_file, \n",
    "                                s3_bucket='qml-solutions-new-york', \n",
    "                                s3_key='XXXXXXXXXXXXXXX',\n",
    "                                aws_access_key_id=credentials.aws_access_key_id, \n",
    "                                aws_secret_access_key=credentials.aws_secret_access_key,\n",
    "                                verbose=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get the Factset Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_max_assets_in_usd = False\n",
    "\n",
    "if build_max_assets_in_usd:\n",
    "\n",
    "    # ASSEMBLE ASSETS IN USD ACROSS THE ENTIRE FSYM_ID UNIVERSE\n",
    "    output_folder = '/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_annual_assets_in_usd/'\n",
    "    fsym_id_list = output_files = os.listdir(output_folder)\n",
    "    fsym_id_list = [f.replace('.csv', '') for f in fsym_id_list]\n",
    "    print('file count:', len(fsym_id_list))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    results = []\n",
    "    for fsym_id in tqdm.tqdm(fsym_id_list):\n",
    "\n",
    "        if counter == 100:\n",
    "            pass\n",
    "        counter +=1 \n",
    "\n",
    "        if fsym_id[0] == '.':\n",
    "            print('error:', fsym_id)\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(output_folder + f'{fsym_id}.csv')\n",
    "        results.append([fsym_id, df['value'].max()])\n",
    "\n",
    "    max_assets_in_usd_df = pd.DataFrame(results, columns=['fsym_id', 'max_assets_in_usd'])\n",
    "    max_assets_in_usd_df.to_csv('/Users/joeybortfeld/Downloads/max_assets_in_usd_by_fsym_id.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full factset universe of companies (primary issues) and the S&P500 subset\n",
    "\n",
    "factset_universe = pd.read_excel('/Users/joeybortfeld/Documents/CreditGradients Data/factset_primary_issue_universe.xlsx', skiprows=4)\n",
    "\n",
    "rename_dict = {'Sec is Primary Issue': 'ff_iscomp',\n",
    "               'Company': 'ff_co_name',\n",
    "               'FactSet Econ Sector': 'factset_econ_sector',\n",
    "               'FactSet Ind': 'factst_industry',\n",
    "               'Bus Desc': 'ff_bus_desc_ext',\n",
    "               'Entity ID': 'factset_entity_id',\n",
    "               'Entity Country HQ': 'factset_hq_country',\n",
    "               'Perm. Sec. ID': 'fsym_id',\n",
    "               'Max Assets (USD)': 'max_assets_in_usd',\n",
    "               'Exchange Couuntry Name': 'exchange_country',\n",
    "               'Exchange Ticker': 'exchange_ticker',\n",
    "               'Ult Parent ID': 'ultimate_parent_id',\n",
    "               'Primary Equity Listing': 'primary_equity_listing',\n",
    "               'P_SYMBOL': 'ticker'}\n",
    "\n",
    "factset_universe = factset_universe.rename(columns=rename_dict)\n",
    "factset_universe = factset_universe[factset_universe['fsym_id'] != '@NA']\n",
    "\n",
    "# sort largest to smallest by total assets (USD)\n",
    "factset_universe = factset_universe.sort_values(by='max_assets_in_usd', ascending=False)\n",
    "factset_universe = factset_universe.reset_index(drop=True)\n",
    "\n",
    "print('Total primary issue universe size:', len(factset_universe))\n",
    "\n",
    "print('--count with max assets > $10B:', len(factset_universe[factset_universe['max_assets_in_usd'] > 10_000]))\n",
    "print('--count with max assets > $5B:', len(factset_universe[factset_universe['max_assets_in_usd'] > 5_000]))\n",
    "print('--count with max assets > $1B:', len(factset_universe[factset_universe['max_assets_in_usd'] > 1_000]))\n",
    "print('--count with max assets > $500M:', len(factset_universe[factset_universe['max_assets_in_usd'] > 500]))\n",
    "print('--count with max assets > $100M:', len(factset_universe[factset_universe['max_assets_in_usd'] > 100]))\n",
    "\n",
    "print()\n",
    "\n",
    "################################################################################\n",
    "# get the S&P 500\n",
    "sp500_universe = pd.read_csv('/Users/joeybortfeld/Documents/CreditGradients Data/sp500_constituents.csv')\n",
    "sp500_universe['sp500'] = 1\n",
    "sp500_universe = sp500_universe.rename(columns={'Symbol': 'ticker'})\n",
    "sp500_universe = sp500_universe[['ticker', 'sp500']]\n",
    "\n",
    "# merge the two\n",
    "factset_universe = factset_universe.merge(sp500_universe, on='ticker', how='left')\n",
    "factset_universe['sp500'] = factset_universe['sp500'].fillna(0).astype(int)\n",
    "print('--sp500 count:', len(factset_universe[factset_universe['sp500'] == 1]))\n",
    "\n",
    "################################################################################\n",
    "# BUILD THE UNIVERSE SET AT VARIOUS ASSET THRESHOLDS\n",
    "universe_dict = {}\n",
    "\n",
    "# save the sp500 fsym_ids as a list\n",
    "temp = factset_universe[factset_universe['sp500'] == 1].copy()\n",
    "temp = temp[temp['fsym_id'] != '@NA']\n",
    "sp500_list = list(temp['fsym_id'])\n",
    "sp500_list = sorted(sp500_list)\n",
    "universe_dict['sp500'] = sp500_list\n",
    "\n",
    "# save the full universe fsym_ids as a list\n",
    "temp = factset_universe.copy()\n",
    "temp = temp[temp['fsym_id'] != '@NA']\n",
    "full_list = list(temp['fsym_id'].unique())\n",
    "universe_dict['full'] = full_list\n",
    "\n",
    "# save fsym_id universe at various asset thresholds\n",
    "for this_threshold in [('$10B', 10_000), ('$5B', 5_000), ('$1B', 1_000), ('$500M', 500), ('$250M', 250), ('$100M', 100)]:\n",
    "    temp = factset_universe[factset_universe['max_assets_in_usd'] > this_threshold[1]].copy()\n",
    "    temp = temp[temp['fsym_id'] != '@NA']\n",
    "    temp = temp['fsym_id'].unique()\n",
    "    temp = sorted(temp)\n",
    "    universe_dict[this_threshold[0]] = temp\n",
    "\n",
    "for k,v in universe_dict.items():\n",
    "    print(k,':', len(v))\n",
    "\n",
    "# save the semi-annual fsym_ids\n",
    "df_semi_annual = pd.read_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_semi_annual_fsym_id_list.csv')\n",
    "universe_dict['semi_annual'] = list(df_semi_annual['fsym_id'])\n",
    "\n",
    "# factset_universe.to_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_universe_processed.csv', index=False)\n",
    "\n",
    "print('done all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get the list of metrics in the Fundamentals API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_categories = [\n",
    "    'INCOME_STATEMENT',\n",
    "    'BALANCE_SHEET',\n",
    "    'CASH_FLOW',\n",
    "    'PENSION_AND_POSTRETIREMENT',\n",
    "    'MARKET_DATA',\n",
    "    'MISCELLANEOUS',\n",
    "    'DATES'\n",
    "]\n",
    "metrics_endpoint = 'https://api.factset.com/content/factset-fundamentals/v2/metrics'\n",
    "\n",
    "collection = []\n",
    "for this_category in metric_categories:\n",
    "\n",
    "    print(this_category)\n",
    "\n",
    "    metrics_request = {\"category\": this_category}\n",
    "    headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "    #create a post request\n",
    "    metrics_post = json.dumps(metrics_request)\n",
    "    metrics_response = requests.get(url = metrics_endpoint, \n",
    "                                    data=metrics_post, \n",
    "                                    auth = authorization, \n",
    "                                    headers = headers, \n",
    "                                    verify= False )\n",
    "    print('HTTP Status: {}'.format(metrics_response.status_code))\n",
    "\n",
    "    #create a dataframe from POST request, show dataframe properties\n",
    "    metrics_data = json.loads(metrics_response.text)\n",
    "    metrics_df = pd.DataFrame(metrics_data['data'])\n",
    "    metrics_df['metric_category'] = this_category\n",
    "    print(metrics_df.shape)\n",
    "    \n",
    "    collection.append(metrics_df)\n",
    "\n",
    "metrics_df = pd.concat(collection, ignore_index=True)\n",
    "# metrics_df.to_csv('/Users/joeybortfeld/Documents/CreditGradients Data/all_fundamental_api_metrics_list.csv')\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fundamentals API Download - Assets in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fundamentals API Download - All Metrics in Local Currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = factset_api.batch_fundamental_download(fsym_list=universe_dict['full'],\n",
    "                               field_list=['FF_ASSETS'],\n",
    "                               currency='LOCAL',\n",
    "                               periodicity_list=['annual', 'quarterly' 'semi_annual'],\n",
    "                               start_date='1990-01-01',\n",
    "                               end_date='2024-12-31',\n",
    "                               skip_if_done=True,\n",
    "                               output_folder='/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_assets_in_usd/',\n",
    "                               factset_api_authorization=credentials.factset_api_authorization)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDIVIDUAL FSYM_ID DOWNLOAD METHOD\n",
    "\n",
    "# download parameters\n",
    "download_type_dict = {'annual': ['ANN', 20], \n",
    "                      'quarterly': ['QTR', 10],\n",
    "                      'semi_annual': ['SEMI', 15]}\n",
    "\n",
    "download_type = 'semi_annual'\n",
    "output_folder = f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_{download_type}/'\n",
    "start_date = '1990-01-01'\n",
    "end_date = '2024-12-31'\n",
    "skip_if_done = True\n",
    "\n",
    "################################################################################\n",
    "# define the company set\n",
    "company_set = universe_dict['$1B']\n",
    "if np.NaN in company_set:\n",
    "    company_set.remove(np.NaN)\n",
    "\n",
    "if download_type == 'semi_annual':\n",
    "    semi_annual_set = universe_dict['semi_annual']\n",
    "    company_set = [e for e in company_set if e in semi_annual_set]\n",
    "\n",
    "print('company set size:', len(company_set))\n",
    "\n",
    "# prescreen to see if the fsym_id is already in the output folder\n",
    "if skip_if_done:\n",
    "    print('--skipping files that are already done')\n",
    "    output_files = os.listdir(output_folder)\n",
    "    print('--file count in output folder:', len(output_files))\n",
    "    output_files = [e.replace('.csv', '') for e in output_files]    \n",
    "    company_set = [e for e in company_set if e not in output_files]\n",
    "\n",
    "    print('--new company set size when skipping:', len(company_set))\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# loop through fsym_ids\n",
    "error_list = []\n",
    "collection = []\n",
    "start_time = time.time()\n",
    "for this_fsym in tqdm.tqdm(company_set):\n",
    "\n",
    "    try:\n",
    "        result = utilities.download_fundamentals(id_list=[this_fsym],\n",
    "                                    field_list=fundamentals_var_list,\n",
    "                                    # field_list=['FF_ASSETS',],\n",
    "                                    periodicity=download_type_dict[download_type][0],\n",
    "                                    start_date=start_date,\n",
    "                                    end_date=end_date,\n",
    "                                    currency='LOCAL',\n",
    "                                    verbose=False,\n",
    "                                    authorization=authorization)\n",
    "        response_code, fundamentals_df = result\n",
    "\n",
    "        if response_code != 200:\n",
    "            error_list.append(this_fysm)\n",
    "            print('error:', this_fysm)\n",
    "        else:\n",
    "\n",
    "            fundamentals_df.to_csv(output_folder + f'{this_fsym}.csv', index=False)\n",
    "            collection.append(fundamentals_df)\n",
    "\n",
    "    except:\n",
    "        error_list.append(this_fsym)\n",
    "        print('error:', this_fsym)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "print('done in {}m'.format((time.time() - start_time) / 60))\n",
    "\n",
    "if len(error_list) > 0:\n",
    "    print('errors in download:', len(error_list))\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('No errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD ASSETS IN USD\n",
    "\n",
    "\n",
    "download_type_dict = {'annual': ['ANN', 20], \n",
    "                      'quarterly': ['QTR', 10],\n",
    "                      'semi_annual': ['SEMI', 15]}\n",
    "\n",
    "download_type = 'semi_annual'\n",
    "start_date = '1990-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "id_batches = batch_a_list(full_list, 30)\n",
    "error_list = []\n",
    "collection = []\n",
    "start_time = time.time()\n",
    "print('batch count:', len(id_batches))\n",
    "for counter, this_batch in enumerate(id_batches):\n",
    "\n",
    "    if counter % 250 == 0:\n",
    "        print(counter, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    counter += 1\n",
    "\n",
    "    result = utilities.download_fundamentals(id_list=this_batch,\n",
    "                                    field_list=['FF_ASSETS'],\n",
    "                                    periodicity=download_type_dict[download_type][0],\n",
    "                                    start_date=start_date,\n",
    "                                    end_date=end_date,\n",
    "                                    currency='USD',\n",
    "                                    verbose=False,\n",
    "                                    authorization=authorization)\n",
    "    response_code, fundamentals_df = result\n",
    "\n",
    "    if response_code != 200:\n",
    "        error_list.append([counter,this_batch])\n",
    "    else:\n",
    "        fundamentals_df.to_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_{download_type}_assets_in_usd/fundamentals_df_{counter}.csv', index=False)\n",
    "        collection.append(fundamentals_df)\n",
    "\n",
    "print('done in {}m'.format((time.time() - start_time) / 60))\n",
    "if len(error_list) > 0:\n",
    "    print('errors in download:')\n",
    "    print(error_list)\n",
    "else:\n",
    "    print('No errors')\n",
    "\n",
    "# combine results into one dataframe\n",
    "df = pd.concat(collection, ignore_index=True)\n",
    "df.to_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_{download_type}_assets_in_usd/fundamentals_df_combined.csv', index=False)\n",
    "\n",
    "if download_type == 'semi_annual':\n",
    "\n",
    "    # save fsym_ids that report semi-annual data\n",
    "    temp = df[df['value'].notnull()]\n",
    "    temp = temp[temp['fsymId'] != '@NA']\n",
    "    temp['fsymId'].nunique()\n",
    "    temp = temp.drop_duplicates(subset=['fsymId'])\n",
    "    temp = temp[['fsymId']]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    temp.columns=['fsym_id']\n",
    "    temp.to_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_semi_annual_fsym_id_list.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING\n",
    "df_annual = pd.read_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_annual/fundamentals_df_combined.csv')\n",
    "df_annual = utilities.preprocess_factset_fundamentals(df_annual, verbose=True)\n",
    "\n",
    "df_quarterly = pd.read_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_quarterly/fundamentals_df_combined.csv')\n",
    "df_quarterly = utilities.preprocess_factset_fundamentals(df_quarterly, verbose=True)\n",
    "\n",
    "df_semi_annual = pd.read_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_semi_annual/fundamentals_df_combined.csv')\n",
    "df_semi_annual = utilities.preprocess_factset_fundamentals(df_semi_annual, verbose=True)\n",
    "\n",
    "# check for any columns that are not in the flow or stock variable lists\n",
    "temp = [c for c in df_annual.columns if c not in utilities.flow_var_list + utilities.stock_var_list]    \n",
    "print('data validation:')\n",
    "print('non flow/stock vars:', temp)\n",
    "print()\n",
    "\n",
    "df_annual_formatted = utilities.format_annual_data(df_annual, \n",
    "                                         flow_vars=utilities.flow_var_list, \n",
    "                                         stock_vars=utilities.stock_var_list, \n",
    "                                         verbose=True)\n",
    "\n",
    "df_quarterly_formatted = utilities.format_quarterly_data(df_quarterly, \n",
    "                                              flow_vars=utilities.flow_var_list, \n",
    "                                              stock_vars=utilities.stock_var_list, \n",
    "                                              verbose=True) \n",
    "\n",
    "df_semi_annual_formatted = utilities.format_semi_annual_data(df_semi_annual, \n",
    "                                              flow_vars=utilities.flow_var_list, \n",
    "                                              stock_vars=utilities.stock_var_list, \n",
    "                                              verbose=True) \n",
    "\n",
    "df_merged = utilities.merge_quarterly_semi_and_annual(quarterly=df_quarterly_formatted, \n",
    "                                             semi_annual=df_semi_annual_formatted, \n",
    "                                             annual=df_annual_formatted, \n",
    "                                             flow_vars=utilities.flow_var_list, \n",
    "                                             stock_vars=utilities.stock_var_list, \n",
    "                                             cleanup=True)\n",
    "\n",
    "\n",
    "# construct ratios\n",
    "df = utilities.build_qml_model_ratios(df_merged, verbose=True)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('/Users/joeybortfeld/Downloads/Default list.csv')\n",
    "\n",
    "# collect all isins for each default\n",
    "temp1 = temp.groupby(['description', 'defaulttype1', 'defaultdate'])[['isin', 'cusip', 'ticker' ]].agg(lambda x: set(x))\n",
    "temp1 = temp1.reset_index()\n",
    "\n",
    "\n",
    "temp = temp.drop_duplicates(subset=['description', 'defaulttype1', 'defaultdate'])\n",
    "temp['first_default_date'] = temp.groupby(['description'])['defaultdate'].transform('min')\n",
    "temp = temp.sort_values(by=['first_default_date', 'description'])\n",
    "temp = temp.reset_index(drop=True)\n",
    "\n",
    "temp = temp[['description', 'defaulttype1', 'defaultdate', 'SectorLevel3', 'SectorLevel4', 'Commodity Sector']]\n",
    "temp = temp.merge(temp1, on=['description', 'defaulttype1', 'defaultdate'], how='left')\n",
    "\n",
    "temp.to_csv('/Users/joeybortfeld/Documents/CreditGradients Data/default_list_unique.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download company profile data\n",
    "\n",
    "company_set = universe_dict['full']\n",
    "if np.NaN in company_set:\n",
    "    company_set.remove(np.NaN)\n",
    "\n",
    "# error_list = []\n",
    "# collection = []\n",
    "error_list2 = []\n",
    "for this_fsym in tqdm.tqdm(error_list):\n",
    "\n",
    "    result = utilities.download_company_profile(id_list=[this_fsym,], authorization=authorization, verbose=False)\n",
    "    response_code, df = result\n",
    "    if response_code != 200:\n",
    "        error_list2.append(this_fsym)\n",
    "    else:\n",
    "        collection.append(df)\n",
    "\n",
    "if len(error_list2) > 0:\n",
    "    print('errors in download:', len(error_list2))\n",
    "\n",
    "else:\n",
    "    print('No errors')\n",
    "\n",
    "    df = pd.concat(collection, ignore_index=True)\n",
    "    df.to_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_company_profiles.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('errors in download:', len(error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fundamentals descriptive data\n",
    "\n",
    "descriptive_var_list = [\n",
    "    'FF_CO_NAME',\n",
    "\n",
    "    'FF_COUNTRY',\n",
    "\n",
    "    'FF_BUS_DESC_ABBREV',\n",
    "    'FF_BUS_DESC_EXT',\n",
    "    \n",
    "    'FF_ISCOMP',\n",
    "    'FF_SECACT',\n",
    "    'FF_ACQ_DATE',\n",
    "\n",
    "    'FF_GEN_IND', \n",
    "    'FF_MAJOR_IND',\n",
    "    'FF_MAJOR_SUBIND',\n",
    "\n",
    "  'FF_MAJOR_IND_NAME',\n",
    "  'FF_MAJOR_SUBIND_NAME']\n",
    "collection = []\n",
    "for this_metric in [[e] for e in descriptive_var_list]:\n",
    "\n",
    "  fundamentals_endpoint = 'https://api.factset.com/content/factset-fundamentals/v2/fundamentals'\n",
    "  fundamentals_request_2={\n",
    "    \"data\": {\n",
    "      \"ids\": ['AAPL-US', 'MSFT-US'],\n",
    "      \"metrics\": this_metric\n",
    "    }\n",
    "  }\n",
    "  headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "  #create a post request\n",
    "  fundamentals_post = json.dumps(fundamentals_request_2)\n",
    "  # print('POST request:')\n",
    "  # print(fundamentals_endpoint)\n",
    "  # print(fundamentals_post)\n",
    "  # print()\n",
    "\n",
    "  fundamentals_response = requests.post(url = fundamentals_endpoint, \n",
    "                                        data=fundamentals_post, \n",
    "                                        auth = authorization, \n",
    "                                        headers = headers, \n",
    "                                        verify= False )\n",
    "  # print('HTTP Status: {}'.format(fundamentals_response.status_code))\n",
    "\n",
    "  # create a dataframe from POST request, show dataframe properties\n",
    "  fundamentals_data = json.loads(fundamentals_response.text)\n",
    "  if fundamentals_response.status_code != 200:\n",
    "    print(fundamentals_data)\n",
    "  else:\n",
    "    df = pd.DataFrame(fundamentals_data['data'])\n",
    "    # print('RECORDS:',len(fundamentals_df))\n",
    "    # print(fundamentals_df[['metric', 'value']].head(20))\n",
    "    df = df.pivot(index=['requestId', 'fsymId'], columns='metric', values='value')\n",
    "    df.reset_index(inplace=True)\n",
    "    collection.append(df)\n",
    "\n",
    "df = pd.concat(collection, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://api.factset.com/content/factset-fundamentals/v2/company-reports/profile?ids=AAPL-US,MSFT-US'\n",
    "\n",
    "request={\n",
    "    'data': {\n",
    "        \"ids\": [\"AAPL-US\"],\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n",
    "headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "post = json.dumps(request)\n",
    "response = requests.get(url = endpoint, auth = authorization, headers = headers, verify= False )\n",
    "temp = pd.DataFrame(json.loads(response.text)['data'])\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = ['AAPL-US', 'MSFT-US']\n",
    "endpoint = 'https://api.factset.com/content/factset-fundamentals/v2/company-reports/profile?ids=' + ','.join(id_list)\n",
    "headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "response = requests.get(url = endpoint, auth = authorization, headers = headers, verify= False )\n",
    "temp = pd.DataFrame(json.loads(response.text)['data'])\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "#create a post request\n",
    "fundamentals_post_3 = json.dumps(fundamentals_request_3)\n",
    "fundamentals_response_3 = requests.post(url = fundamentals_endpoint, data=fundamentals_post_3, auth = authorization, headers = headers, verify= False )\n",
    "print('HTTP Status: {}'.format(fundamentals_response_3.status_code))\n",
    "\n",
    "#create a dataframe from POST request, show dataframe properties\n",
    "fundamentals_data_3 = json.loads(fundamentals_response_3.text)\n",
    "# fundamentals_df_3 = json_normalize(fundamentals_data_3['data'])\n",
    "fundamentals_df_3 = pd.DataFrame(fundamentals_data_3['data'])\n",
    "print('COLUMNS:')\n",
    "print('')\n",
    "print(fundamentals_df_3.dtypes)\n",
    "print('')\n",
    "print('RECORDS:',len(fundamentals_df_3))\n",
    "\n",
    "#show the last 5 records for select columns\n",
    "fundamentals_df_3[['requestId','fsymId','metric','periodicity','fiscalPeriod','periodicity','fiscalYear','fiscalPeriodLength','fiscalEndDate','reportDate','epsReportDate','updateType','currency','value']].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file = 's3://qml-solutions-new-york/factset-api-fundamentals-annual/fundamentals_df_0.csv'\n",
    "temp = pd.read_csv(file, storage_options=aws_credentials)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONVERSION FROM BATCHES TO SINGLE FSYM_ID PER FILE\n",
    "\n",
    "counter = 0\n",
    "for i in range(1,3275):\n",
    "\n",
    "    if counter % 250 == 0:\n",
    "        print(counter, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    file = f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_semi_annual_assets_in_usdX/fundamentals_df_{i}.csv'\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    fsym_ids = list(df['fsymId'].unique())\n",
    "    for fsym_id in fsym_ids:\n",
    "        df[df['fsymId'] == fsym_id].to_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_semi_annual_assets_in_usd/{fsym_id}.csv', index=False)\n",
    "\n",
    "    counter += 1    \n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLECT ASSETS IN USD\n",
    "df = pd.read_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_annual_assets_in_usd/fundamentals_df_combined.csv')\n",
    "print(df.columns)\n",
    "print(df['fsymId'].nunique())\n",
    "\n",
    "fsfsd\n",
    "\n",
    "df = df[df['fsymId'] != '@NA']\n",
    "df = df[df['value'].notnull()]\n",
    "df = df.sort_values(by=['fsymId', 'value'], ascending=[True, False])\n",
    "df = df.drop_duplicates(subset=['fsymId'], keep='first')\n",
    "df = df.sort_values(by='value', ascending=False)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df = df[['fsymId', 'currency', 'fiscalEndDate', 'value']]\n",
    "df.to_csv('/Users/joeybortfeld/Downloads/temp.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EQUITY DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDIVIDUAL FSYM_ID DOWNLOAD METHOD\n",
    "\n",
    "metric = 'returns' # either 'prices' or 'returns'\n",
    "split = 'SPLIT'\n",
    "start_date = '2006-01-03'\n",
    "end_date = '2024-11-12'\n",
    "skip_if_done = True\n",
    "\n",
    "assert metric in ['prices', 'returns'], 'error: metric must be either price or return'\n",
    "\n",
    "if metric == 'prices':\n",
    "    output_folder = f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_{split.lower()}/'\n",
    "else:\n",
    "    output_folder = f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_returns/'\n",
    "print('writing to', output_folder)\n",
    "\n",
    "################################################################################\n",
    "# define the company set\n",
    "company_set = universe_dict['$1B']\n",
    "if np.NaN in company_set:\n",
    "    company_set.remove(np.NaN)\n",
    "print('company set size:', len(company_set))\n",
    "\n",
    "# prescreen to see if the fsym_id is already in the output folder\n",
    "if skip_if_done:\n",
    "    print('--skipping files that are already done')\n",
    "    output_files = os.listdir(output_folder)\n",
    "    print('--file count in output folder:', len(output_files))\n",
    "    output_files = [e.replace('.csv', '') for e in output_files]    \n",
    "    company_set = [e for e in company_set if e not in output_files]\n",
    "\n",
    "    print('--new company set size when skipping:', len(company_set))\n",
    "################################################################################\n",
    "\n",
    "# loop through fsym_ids\n",
    "error_list = []\n",
    "collection = []\n",
    "start_time = time.time()\n",
    "for this_fsym in tqdm.tqdm(company_set):\n",
    "\n",
    "    try:\n",
    "        if metric == 'prices':\n",
    "            result = utilities.get_stock_prices(id_list=[this_fsym],\n",
    "                                        field_list=[\"price\", \"volume\", \"tradeCount\"],\n",
    "                                        start_date=start_date,\n",
    "                                        end_date=end_date,\n",
    "                                        frequency='D',\n",
    "                                        verbose=False, \n",
    "                                        split=split,\n",
    "                                        authorization=authorization)\n",
    "        else:\n",
    "            result = utilities.get_stock_returns(id_list=[this_fsym],\n",
    "                                        start_date=start_date,\n",
    "                                        end_date=end_date,\n",
    "                                        frequency='D',\n",
    "                                        verbose=False, \n",
    "                                        authorization=authorization)\n",
    "        response_code, this_df = result\n",
    "\n",
    "        if response_code != 200:\n",
    "            error_list.append(this_fysm)\n",
    "            # print('error:', this_fysm)\n",
    "        else:\n",
    "\n",
    "            this_df.to_csv(output_folder + f'{this_fsym}.csv', index=False)\n",
    "            # collection.append(this_df)\n",
    "\n",
    "    except:\n",
    "        error_list.append(this_fsym)\n",
    "        print('error:', this_fsym)\n",
    "\n",
    "print('done in {}m'.format((time.time() - start_time) / 60))\n",
    "\n",
    "if len(error_list) > 0:\n",
    "    print('errors in download:')\n",
    "    print(len(error_list))\n",
    "    print(error_list)\n",
    "else:\n",
    "    print('No errors')\n",
    "\n",
    "# df = pd.concat(collection, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = utilities.get_stock_prices(id_list=['MH33D6-R'],\n",
    "                                        field_list=[\"price\", \"volume\", \"tradeCount\"],\n",
    "                                        start_date='2024-01-01',\n",
    "                                        end_date='2024-10-31',\n",
    "                                        frequency='D',\n",
    "                                        verbose=False, \n",
    "                                        split='SPLIT',\n",
    "                                        authorization=authorization)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDIVIDUAL FSYM_ID DOWNLOAD METHOD - SHARES OUTSTANDING\n",
    "\n",
    "# download parameters\n",
    "download_type_dict = {'annual': ['ANN', 20], \n",
    "                      'quarterly': ['QTR', 10],\n",
    "                      'semi_annual': ['SEMI', 15]}\n",
    "\n",
    "download_type = 'quarterly'\n",
    "output_folder = f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_fundamentals_shares_outstanding_{download_type}/'\n",
    "start_date = '1990-01-01'\n",
    "end_date = '2024-12-31'\n",
    "skip_if_done = True\n",
    "\n",
    "print('writing to', output_folder)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# define the company set\n",
    "company_set = universe_dict['$1B']\n",
    "if np.NaN in company_set:\n",
    "    company_set.remove(np.NaN)\n",
    "\n",
    "if download_type == 'semi_annual':\n",
    "    semi_annual_set = universe_dict['semi_annual']\n",
    "    company_set = [e for e in company_set if e in semi_annual_set]\n",
    "\n",
    "print('company set size:', len(company_set))\n",
    "\n",
    "# prescreen to see if the fsym_id is already in the output folder\n",
    "if skip_if_done:\n",
    "    print('--skipping files that are already done')\n",
    "    output_files = os.listdir(output_folder)\n",
    "    print('--file count in output folder:', len(output_files))\n",
    "    output_files = [e.replace('.csv', '') for e in output_files]    \n",
    "    company_set = [e for e in company_set if e not in output_files]\n",
    "\n",
    "    print('--new company set size when skipping:', len(company_set))\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# loop through fsym_ids\n",
    "error_list = []\n",
    "collection = []\n",
    "start_time = time.time()\n",
    "print('start time:', datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "for this_fsym in tqdm.tqdm(company_set):\n",
    "\n",
    "    try:\n",
    "        result = utilities.download_fundamentals(id_list=[this_fsym],\n",
    "                                    field_list=['FF_COM_SHS_OUT',],\n",
    "                                    periodicity=download_type_dict[download_type][0],\n",
    "                                    start_date=start_date,\n",
    "                                    end_date=end_date,\n",
    "                                    currency='LOCAL',\n",
    "                                    verbose=False,\n",
    "                                    authorization=authorization)\n",
    "        response_code, fundamentals_df = result\n",
    "\n",
    "        if response_code != 200:\n",
    "            error_list.append(this_fysm)\n",
    "            print('error:', this_fysm)\n",
    "        else:\n",
    "\n",
    "            fundamentals_df.to_csv(output_folder + f'{this_fsym}.csv', index=False)\n",
    "            collection.append(fundamentals_df)\n",
    "\n",
    "    except:\n",
    "        error_list.append(this_fsym)\n",
    "        print('error:', this_fsym)\n",
    "\n",
    "print('done in {}m'.format((time.time() - start_time) / 60))\n",
    "\n",
    "if len(error_list) > 0:\n",
    "    print('errors in download:')\n",
    "    print(error_list)\n",
    "else:\n",
    "    print('No errors')\n",
    "print('end time:', datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE PUBLIC UNIVERSE\n",
    "# these are companies where we have a price\n",
    "\n",
    "output_dir = '/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_split/'\n",
    "file_list = os.listdir(output_dir)\n",
    "file_list = [f for f in file_list if f.endswith('.csv')]\n",
    "\n",
    "print(f'There are {len(file_list)} files in the directory')\n",
    "\n",
    "collection = []\n",
    "for f in tqdm.tqdm(file_list):\n",
    "    df = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_split/{f}')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df[['fsymId', 'date', 'price']]\n",
    "    df = df[df['price'].notnull()]\n",
    "    min_date  = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    collection.append([f.replace('.csv', ''), min_date, max_date])\n",
    "\n",
    "df = pd.DataFrame(collection, columns=['fsym_id', 'min_date', 'max_date'])\n",
    "df['min_date'] = pd.to_datetime(df['min_date'])\n",
    "df['max_date'] = pd.to_datetime(df['max_date'])\n",
    "\n",
    "df.to_csv('/Users/joeybortfeld/Documents/CreditGradients Data/factset_public_universe_with_assets_greater_than_1_billion_usd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD MONTH END STOCK PRICES HISTORY\n",
    "\n",
    "input_dir = '/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_split/'\n",
    "output_dir = '/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_split_month_end/'\n",
    "file_list = os.listdir(input_dir)\n",
    "file_list = [f for f in file_list if f.endswith('.csv')]\n",
    "\n",
    "print(f'There are {len(file_list)} files in the directory')\n",
    "\n",
    "collection = []\n",
    "for f in tqdm.tqdm(file_list):\n",
    "    df = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_split/{f}')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    df = df[df['price'].notnull()]\n",
    "\n",
    "    # only keep the month end prices (not exactly month end, if a stock only trades on the 10th of the month, it will be included)\n",
    "    df['max_date'] = df.groupby(['fsymId', 'year', 'month'])['date'].transform('max')\n",
    "    df = df[df['date'] == df['max_date']]\n",
    "    \n",
    "    df = df[['fsymId', 'currency', 'year', 'month', 'date', 'price']]\n",
    "\n",
    "    \n",
    "\n",
    "    df.to_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_split_month_end/{f}', index=False)\n",
    "\n",
    "\n",
    "    # save to /Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices_month_end/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.build_market_cap(fsym_id='MH33D6-R', market_cap_type='daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE MARKET CAP\n",
    "# use month end price data and combine with shares outstanding data\n",
    "\n",
    "# market cap parameters\n",
    "market_cap_type = 'daily'\n",
    "ffill_limit_dict = {'monthly': 16, 'daily': 375}\n",
    "assert market_cap_type in ['monthly', 'daily'], 'error: market_cap_type must be either monthly or daily'\n",
    "this_fsym = 'MH33D6-R'\n",
    "this_fsym='HTM0LK-R'\n",
    "\n",
    "\n",
    "# build the set of fsym_ids with semi annual share data\n",
    "semi_annual_share_dir = '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_api_fundamentals_shares_outstanding_semi_annual/'\n",
    "semi_annual_share_list = os.listdir(semi_annual_share_dir)\n",
    "semi_annual_share_list = [f for f in semi_annual_share_list if f.endswith('.csv')]\n",
    "semi_annual_share_list = [f.replace('.csv', '') for f in semi_annual_share_list ]\n",
    "\n",
    "\n",
    "# 0. get price data\n",
    "if market_cap_type == 'monthly':\n",
    "    df1 = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_api_stock_prices_split_month_end/{this_fsym}.csv')\n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    df1 = df1[['fsymId', 'date', 'price', 'currency']]\n",
    "else:\n",
    "    df1 = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_api_stock_prices_split/{this_fsym}.csv')\n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    df1 = df1[['fsymId', 'date', 'price', 'currency']]\n",
    "\n",
    "# 1. get shares outstanding\n",
    "df2 = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_api_fundamentals_shares_outstanding_quarterly/{this_fsym}.csv')\n",
    "df2['fiscalEndDate'] = pd.to_datetime(df2['fiscalEndDate'])\n",
    "df2['epsReportDate'] = pd.to_datetime(df2['epsReportDate'])\n",
    "df2['epsReportDate'] = df2['epsReportDate'].fillna(df2['fiscalEndDate'] + pd.Timedelta(days=90))\n",
    "df2 = df2[['fsymId', 'epsReportDate', 'value']]\n",
    "df2.columns=['fsymId', 'date', 'shares_outstanding_quarterly']\n",
    "\n",
    "df3 = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_api_fundamentals_shares_outstanding_annual/{this_fsym}.csv')\n",
    "df3['fiscalEndDate'] = pd.to_datetime(df3['fiscalEndDate'])\n",
    "df3['epsReportDate'] = pd.to_datetime(df3['epsReportDate'])\n",
    "df3['epsReportDate'] = df3['epsReportDate'].fillna(df3['fiscalEndDate'] + pd.Timedelta(days=90))\n",
    "df3 = df3[['fsymId', 'epsReportDate', 'value']]\n",
    "df3.columns=['fsymId', 'date', 'shares_outstanding_annual']\n",
    "\n",
    "# combine all shares outstanding data\n",
    "df4 = df2.merge(df3, how='outer', on=['fsymId', 'date'])\n",
    "\n",
    "# check if semi annual data exists\n",
    "if this_fsym in semi_annual_share_list:\n",
    "    df5 = pd.read_csv(f'/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data/factset_api_fundamentals_shares_outstanding_semi_annual/{this_fsym}.csv')\n",
    "    df5['fiscalEndDate'] = pd.to_datetime(df5['fiscalEndDate'])\n",
    "    df5['epsReportDate'] = pd.to_datetime(df5['epsReportDate'])\n",
    "    df5['epsReportDate'] = df5['epsReportDate'].fillna(df5['fiscalEndDate'] + pd.Timedelta(days=90))\n",
    "    df5 = df5[['fsymId', 'epsReportDate', 'value']]\n",
    "    df5.columns=['fsymId', 'date', 'shares_outstanding_semi_annual']\n",
    "\n",
    "    df4 = df4.merge(df5, how='outer', on=['fsymId', 'date'])\n",
    "\n",
    "# 2. merge shares outstanding with monthly prices\n",
    "df = df1.merge(df4, how='outer', on=['fsymId', 'date'])\n",
    "\n",
    "# 3. data cleaning\n",
    "# get the date of the first observation with non null price\n",
    "first_date = df[df['price'].notnull()]['date'].min()\n",
    "df = df[df['date'] >= first_date]\n",
    "\n",
    "# fill forward shares outstanding\n",
    "df = df.sort_values(by=['date'])\n",
    "df['shares_outstanding'] = df['shares_outstanding_quarterly'].fillna(df['shares_outstanding_annual'])\n",
    "df['shares_outstanding'] = df['shares_outstanding'].fillna(method='ffill', limit=ffill_limit_dict[market_cap_type])   # allow 12 months of fill forward (12 row) plut 4 more rows for quarterly financial filings\n",
    "\n",
    "# 4. market cap calculation (millions)\n",
    "df['market_cap'] = df['price'] * df['shares_outstanding']\n",
    "\n",
    "# 5. cleanup\n",
    "df = df[df['price'].notnull()]\n",
    "df = df[['fsymId', 'date', 'market_cap', 'price', 'shares_outstanding']]\n",
    "df.columns = ['fsym_id', 'market_cap_date', 'market_cap', 'price', 'shares_outstanding']\n",
    "df = df.sort_values(by=['fsym_id', 'market_cap_date'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['year'] = df['market_cap_date'].dt.year\n",
    "df['month'] = df['market_cap_date'].dt.month\n",
    "\n",
    "df.set_index('market_cap_date')['market_cap'].plot()\n",
    "\n",
    "\n",
    "\n",
    "df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Files from Local Directory to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "def s3_check_file_exists(bucket_name:str='qml-solutions-new-york', \n",
    "                         file_key:str='/factset-api-global-prices/B01DPB-R.csv', \n",
    "                         aws_access_key_id:str=None, \n",
    "                         aws_secret_access_key:str=None):\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "    )\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "        return True\n",
    "    except s3.exceptions.ClientError:\n",
    "        return False\n",
    "\n",
    "def copy_file_to_s3(local_file_path:str=None, \n",
    "                     s3_bucket:str='qml-solutions-new-york', \n",
    "                     s3_key:str='factset-api-global-prices/', \n",
    "                     aws_access_key_id:str=None, \n",
    "                     aws_secret_access_key:str=None,\n",
    "                     verbose:bool=False):\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "    )   \n",
    "    try:\n",
    "        s3.upload_file(local_file_path, s3_bucket, s3_key)\n",
    "        if verbose:\n",
    "            print(f'--uploaded {local_file_path} to {s3_bucket}/{s3_key}')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# loop through files in local folder\n",
    "# Transfer file to AWS\n",
    "\n",
    "s3_bucket = 'qml-solutions-new-york'\n",
    "\n",
    "\n",
    "local_dir = '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data'\n",
    "error_dict = {}\n",
    "for source_folder in [\n",
    "    # 'factset_api_fundamentals_quarterly', \n",
    "    # 'factset_api_fundamentals_annual',\n",
    "\n",
    "    'factset_api_fundamentals_semi_annual',\n",
    "    'factset_api_stock_prices_split',\n",
    "    'factset_api_stock_returns',\n",
    "    'factset_api_fundamentals_shares_outstanding_quarterly',\n",
    "    'factset_api_fundamentals_shares_outstanding_annual',\n",
    "    'factset_api_fundamentals_shares_outstanding_semi_annual',\n",
    "    'factset_api_fundamentals_annual_assets_in_usd',\n",
    "    'factset_api_fundamentals_semi_annual_assets_in_usd',\n",
    "\n",
    "    ]:\n",
    "\n",
    "    print(source_folder)\n",
    "    target_folder = source_folder.replace('_', '-')\n",
    "    file_list = os.listdir(local_dir + '/' + source_folder + '/')\n",
    "    error_list = []\n",
    "    print('file count:', len(file_list))\n",
    "    print('start at:', datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    for file in tqdm.tqdm(file_list):\n",
    "\n",
    "        does_exist = s3_check_file_exists(bucket_name=s3_bucket, \n",
    "                            file_key=f'{target_folder}/{file}', \n",
    "                            aws_access_key_id=aws_credentials['key'], \n",
    "                            aws_secret_access_key=aws_credentials['secret'])\n",
    "\n",
    "        if not does_exist:\n",
    "\n",
    "            response = copy_file_to_s3(local_file_path=f'{local_dir}/{source_folder}/{file}', \n",
    "                                s3_bucket=s3_bucket, \n",
    "                            s3_key=f'{target_folder}/{file}', \n",
    "                            aws_access_key_id=aws_credentials['key'], \n",
    "                            aws_secret_access_key=aws_credentials['secret'],\n",
    "                            verbose=False)\n",
    "\n",
    "            if not response:\n",
    "                error_list.append(file)\n",
    "    \n",
    "    print('error count:', len(error_list))\n",
    "    print()\n",
    "    error_dict[source_folder] = error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTITHREAD BULK UPLOAD FROM LOCAL TO S3\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "\n",
    "\n",
    "def upload_file_to_s3(local_file_path, bucket_name, s3_key, s3_client):\n",
    "    \"\"\"\n",
    "    Uploads a single file to S3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "        return True\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error uploading {local_file_path} to {bucket_name}/{s3_key}: {e}\")\n",
    "        return False\n",
    "\n",
    "def bulk_upload_to_s3(local_dir, local_folder, bucket_name, aws_access_key_id, aws_secret_access_key, num_threads=8):\n",
    "    \"\"\"\n",
    "    Uploads all files in local_dir to the specified S3 bucket.\n",
    "    \"\"\"\n",
    "    # Initialize S3 client \n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "    # Collect a list of local file names to transfer (aka 'MH33D6-R.csv', ''XQCWLZ-R.csv)\n",
    "\n",
    "    target_folder = local_folder.replace('_', '-')\n",
    "    local_file_list = os.listdir(local_dir + '/' + local_folder + '/')\n",
    "\n",
    "    local_path_list = [f'{local_dir}/{local_folder}/{f}' for f in local_file_list]\n",
    "    s3_key_list = [f'{target_folder}/{f}' for f in local_file_list]\n",
    "\n",
    "    from_to_list = list(zip(local_path_list, s3_key_list))\n",
    "\n",
    "    # diagnostics\n",
    "    print(f'transfer files from {local_dir}/{local_folder}')\n",
    "    print(f'transfer to s3 {bucket_name}/{target_folder}')\n",
    "    print('files to transfer:', len(from_to_list))\n",
    "\n",
    " \n",
    "    # Use ThreadPoolExecutor for parallel uploads\n",
    "    error_list = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for local_path, s3_key in from_to_list:\n",
    "            futures.append(\n",
    "                executor.submit(upload_file_to_s3, local_path, bucket_name, s3_key, s3_client)\n",
    "            )\n",
    "\n",
    "        # Track progress with tqdm\n",
    "        for future in tqdm.tqdm(futures, desc=\"Uploading files\"):\n",
    "            try:\n",
    "                if not future.result():\n",
    "                    # Add failed uploads to the list\n",
    "                    error_list.append(futures[future])\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                error_list.append(futures[future])\n",
    "\n",
    "    \n",
    "    # collect and retry errors\n",
    "    print(f\"Failed uploads: {len(error_list)}\")\n",
    "    final_error_list = []\n",
    "    if error_list:\n",
    "        print(\"Retrying failed uploads...\")\n",
    "        for local_path, s3_key in error_list:\n",
    "            success = upload_file_to_s3(local_path, bucket_name, s3_key, s3_client)\n",
    "            if not success:\n",
    "                print(f\"Final failure for {local_path}\")\n",
    "                final_error_list.append(local_path)\n",
    "\n",
    "    print(\"Upload process complete.\")\n",
    "    if len(final_error_list) == 0:\n",
    "        return True, []\n",
    "    else:\n",
    "        return False, final_error_list\n",
    "\n",
    "# Example usage\n",
    "\n",
    "    \n",
    "local_dir = '/Users/joeybortfeld/Documents/CreditGradients Data/Factset Data'\n",
    "bucket_name = 'qml-solutions-new-york'\n",
    "\n",
    "for local_folder in [\n",
    "\n",
    "    # 'factset_api_fundamentals_annual',\n",
    "    # 'factset_api_fundamentals_quarterly',\n",
    "    'factset_api_fundamentals_semi_annual',\n",
    "\n",
    "    'factset_api_fundamentals_annual_assets_in_usd',\n",
    "    'factset_api_fundamentals_semi_annual_assets_in_usd',\n",
    "\n",
    "    'factset_api_fundamentals_shares_outstanding_annual',\n",
    "    'factset_api_fundamentals_shares_outstanding_quarterly',\n",
    "    'factset_api_fundamentals_shares_outstanding_semi_annual',\n",
    "\n",
    "    'factset_api_stock_prices_split',\n",
    "    'factset_api_stock_returns',\n",
    "    ]:\n",
    "\n",
    "    success, error_list = bulk_upload_to_s3(\n",
    "        local_dir=local_dir,\n",
    "        local_folder=local_folder,\n",
    "        bucket_name=bucket_name,\n",
    "        aws_access_key_id=aws_credentials['key'],\n",
    "        aws_secret_access_key=aws_credentials['secret'],\n",
    "        num_threads=8  # Adjust number of threads based on your system's capabilities\n",
    "    )\n",
    "\n",
    "    print(success, error_list)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_check_file_exists(bucket_name:str='qml-solutions-new-york', \n",
    "                         file_key:str='/factset-api-global-prices/B01DPB-R.csv', \n",
    "                         aws_access_key_id:str=None, \n",
    "                         aws_secret_access_key:str=None):\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "    )\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "        return True\n",
    "    except s3.exceptions.ClientError:\n",
    "        return False\n",
    "\n",
    "\n",
    "s3_check_file_exists(bucket_name='qml-solutions-new-york', \n",
    "                         file_key='factset-api-stock-prices-split/MH33D6-R.csv', \n",
    "                         aws_access_key_id=aws_credentials['key'], \n",
    "                         aws_secret_access_key=aws_credentials['secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = pd.read_csv('s3://qml-solutions-new-york/factset-api-global-prices/B01DPB-R.csv',\n",
    "                   storage_options=aws_credentials)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Transfer file to AWS\n",
    "local_folder = '/Users/joeybortfeld/Documents/CreditGradients Data/factset_api_stock_prices/'\n",
    "s3_bucket = 'qml-solutions-new-york'\n",
    "\n",
    "\n",
    "\n",
    "def s3_check_file_exists(bucket_name:str='qml-solutions-new-york', \n",
    "                         file_key:str='/factset-api-global-prices/B01DPB-R.csv', \n",
    "                         aws_access_key_id:str=None, \n",
    "                         aws_secret_access_key:str=None):\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "    )\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "        return True\n",
    "    except s3.exceptions.ClientError:\n",
    "        return False\n",
    "    \n",
    "# CHECK IF FILE EXISTS IN S3\n",
    "res = s3_check_file_exists(bucket_name=s3_bucket, \n",
    "                     file_key='factset-api-global-prices/B01DPB-R.csv', \n",
    "                     aws_access_key_id=aws_credentials['key'], \n",
    "                     aws_secret_access_key=aws_credentials['secret'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def list_s3_bucket_contents(bucket_name, prefix='', aws_access_key_id=None, aws_secret_access_key=None):\n",
    "    \"\"\"\n",
    "    List all items in an S3 bucket and subfolder.\n",
    "    \n",
    "    Parameters:\n",
    "    - bucket_name: str, name of the S3 bucket\n",
    "    - prefix: str, the folder path within the bucket (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - List of file keys (paths) in the specified bucket and folder\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', \n",
    "                             aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,)\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    \n",
    "    file_keys = []\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                file_keys.append(obj['Key'])\n",
    "    \n",
    "    return file_keys\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "bucket_name = 'qml-solutions-new-york'\n",
    "folder_path = 'factset-api-fundamentals-annual/'  # Optional\n",
    "file_list = list_s3_bucket_contents(bucket_name, folder_path, aws_access_key_id=aws_credentials2['key'], aws_secret_access_key=aws_credentials2['secret'])\n",
    "print(len(file_list))\n",
    "print(file_list[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.download_fundamentals(id_list=['MH33D6-R'],\n",
    "                                    field_list=['FF_BUS_DESC_ABBREV'],\n",
    "                                    # periodicity=download_type_dict[download_type][0],\n",
    "                                    # start_date=start_date,\n",
    "                                    # end_date=end_date,\n",
    "                                    # currency='USD',\n",
    "                                    # verbose=False,\n",
    "                                    # authorization=authorization\n",
    "                                    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
