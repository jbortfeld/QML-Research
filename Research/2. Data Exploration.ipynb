{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import scipy.stats\n",
    "from Py_Files import metric_inventory\n",
    "from Py_Files import aws_rds\n",
    "from Py_Files import credentials\n",
    "from Py_Files import data_exploration\n",
    "\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "data_dir = '/Users/joeybortfeld/Documents/QML Solutions Data/'\n",
    "s3_dir = 's3://qml-solutions-new-york/'\n",
    "metric_list = metric_inventory.ratio_dict['size'] + metric_inventory.ratio_dict['leverage'] + metric_inventory.ratio_dict['coverage'] + metric_inventory.ratio_dict['profitability'] + metric_inventory.ratio_dict['liquidity'] + metric_inventory.ratio_dict['volatility']\n",
    "print('ratio count:', len(metric_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir + f'qml_modeling_data/modeling_dataset_with_bankruptcy_labels_us_nonfin_100m_20250212.csv')\n",
    "df['fiscal_end_date'] = pd.to_datetime(df['fiscal_end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Generate Quantile Distribution for Box Plots and Table Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "\n",
    "# build quantile summaries for each ratio across all sectors\n",
    "quantile_list = [0, 0.01, .02, .03, .04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95,0.96,0.97,0.98, 0.99, 1]\n",
    "groupby = 'factset_econ_sector'\n",
    "start = time.time()\n",
    "\n",
    "for m in tqdm.tqdm(metric_list):\n",
    "    \n",
    "    temp = data_exploration.quantile_analysis(df, metric=m, quantile_list=quantile_list, groupby=groupby)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_quantile_summaries/quantile_summary_table_{m}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-quantile-summaries/quantile_summary_table_{m}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "print('done in', time.time() - start)\n",
    "print('--', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Quantile Distribution for Box Plots to Compare Bankruptcy vs Non-Bankruptcy\n",
    "* This generates the box data (25th, 50th, 75th percentiles and more) for observations conditional that they go into bankruptcy 1,2,3,4,5 years out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "\n",
    "sector_groupby = 'factset_econ_sector'\n",
    "collection = []\n",
    "start = time.time()\n",
    "\n",
    "for this_metric in tqdm.tqdm(metric_list):\n",
    "    temp = data_exploration.quantile_analysis_by_default_class(df, this_metric, sector_groupby)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_quantile_summaries_by_default_class/quantile_summary_table_{this_metric}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-quantile-summaries-by-default-class/quantile_summary_table_{this_metric}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "\n",
    "print('done in', time.time() - start)\n",
    "print('--', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate Realized Default Rates by Ratio Deciles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "groupby = 'factset_econ_sector'\n",
    "start = time.time()\n",
    "\n",
    "for this_metric in tqdm.tqdm(metric_list):\n",
    "    temp = data_exploration.default_rate_by_ratio_decile(data=df, metric=this_metric, groupby=groupby)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_default_rates_by_decile/decile_default_rate_{this_metric}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-default-rates-by-decile/decile_default_rate_{this_metric}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "print('done in', time.time() - start)\n",
    "print('--', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Histogram Data for Ratio Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build histogram\n",
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "write_to_rds = True\n",
    "groupby = 'factset_econ_sector'\n",
    "quantile_list = [0, 0.01, .02, .03, .04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95,0.96,0.97,0.98, 0.99, 1]\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "collection = []\n",
    "for this_metric in tqdm.tqdm(metric_list):\n",
    "    temp = data_exploration.generate_histogram_data(df, this_metric, quantiles=(.01, .99), groupby=groupby)\n",
    "    collection.append(temp)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_histograms/ratio_histogram_summary_table_{this_metric}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-histograms/ratio_histogram_summary_table_{this_metric}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "\n",
    "if write_to_rds:\n",
    "    print('writing to rds')\n",
    "    collection = pd.concat(collection, axis=0)\n",
    "\n",
    "    sqlalchemy_engine = aws_rds.sqlalchemy_connect_to_rds(credentials.aws_rds_credentials)\n",
    "    collection.to_sql('ratio_histogram_summary_table', sqlalchemy_engine, if_exists='replace', index=False)\n",
    "    print('done in ', time.time() - start)\n",
    "\n",
    "    # set indices in postgres database table\n",
    "    psycopg2_connection = aws_rds.psycopg2_connect_to_rds(credentials.aws_rds_credentials)\n",
    "    aws_rds.create_index_on_rds(table_name='ratio_histogram_summary_table', \n",
    "                            index_name='idx_metric_sector_lower_clip', \n",
    "                            columns_to_index=['metric', 'sector', 'lower_clip'], \n",
    "                            conn=psycopg2_connection)\n",
    "\n",
    "\n",
    "        \n",
    "print('done in ', time.time() - start)\n",
    "print('--', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bankruptcy Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = data_exploration.build_default_diagnostics(df)\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    print(f'{i}Y defaults with assets/ebitda/cf:', (temp[f'fund_count_{i}'] == 3).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Univariate Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "\n",
    "\n",
    "# apply percentile transformations to ratios using a specified subset for determining the percentile distribution\n",
    "df_model, pct_vars_dict = data_exploration.model_df_prep(df, metric_list=metric_list, test_split_date=None)\n",
    "df_model_split, pct_vars_dict_split = data_exploration.model_df_prep(df, metric_list=metric_list, test_split_date='2018-01-01')\n",
    "\n",
    "\n",
    "for n in range(1,6):\n",
    "    \n",
    "    horizon = n\n",
    "\n",
    "    # with pct transformation = No\n",
    "    #results = univariate_reg(df_model, display_name_dict, horizon, 'No', None)\n",
    "    #results.to_csv(f'/Users/annelilefranc/Documents/QML Files/univariate_reg_{n}y.csv', index=False)\n",
    "    # with train/test split\n",
    "    results = data_exploration.univariate_reg(df_model_split, var_list=metric_list, horizon=n, pct='No', test_split_date=None)\n",
    "    if write_to_local:\n",
    "        results.to_csv(data_dir + f'exploratory_data/default_model_univariate_regressions/univariate_reg_{n}y_split.csv', index=False)\n",
    "    if write_to_s3:\n",
    "        results.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/default-model-univariate-regressions/univariate_reg_{n}y_split.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "    # with pct transformation = Yes\n",
    "    #results = univariate_reg(df_model, pct_vars_dict, horizon, 'Yes', None)\n",
    "    #results.to_csv(f'/Users/annelilefranc/Documents/QML Files/univariate_reg_{n}y-pct.csv', index=False)\n",
    "    # with train/test split\n",
    "    results = data_exploration.univariate_reg(df_model_split, var_list=metric_list, horizon=n, pct='Yes', test_split_date='2018-01-01')\n",
    "    if write_to_local:  \n",
    "        results.to_csv(data_dir + f'exploratory_data/default_model_univariate_regressions/univariate_reg_{n}y-pct_split.csv', index=False)\n",
    "    if write_to_s3: \n",
    "        results.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/default-model-univariate-regressions/univariate_reg_{n}y-pct_split.csv', index=False, storage_options=credentials.aws_s3_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage statistics\n",
    "df.groupby(by='report_date')['ff_assets_in_usd'].count().plot()\n",
    "df.groupby(by='report_date')['market_leverage'].count().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['fsym_id', 'fiscal_end_date', 'default_1', 'default_5', 'bankruptcy_date', 'ff_assets_in_usd', 'total_equity_to_assets', 'net_income_to_sales']].copy()\n",
    "temp['ff_assets_in_usd'] = temp.groupby('fsym_id')['ff_assets_in_usd'].ffill(limit=4)\n",
    "\n",
    "# convert to decile\n",
    "temp['ff_assets_in_usd_decile'] = pd.qcut(temp['ff_assets_in_usd'], q=100, labels=False)\n",
    "temp['total_equity_to_assets_decile'] = pd.qcut(temp['total_equity_to_assets'], q=100, labels=False)\n",
    "temp['net_income_to_sales_decile'] = pd.qcut(temp['net_income_to_sales'], q=100, labels=False)\n",
    "for m in ['ff_assets_in_usd_decile', 'total_equity_to_assets_decile', 'net_income_to_sales_decile']:\n",
    "    temp[m] = temp[m] + 1\n",
    "\n",
    "# adjustments\n",
    "# -- we want small values to be bad and large values to be good\n",
    "pass\n",
    "\n",
    "temp['size_x_leverage'] = temp['ff_assets_in_usd_decile'] * temp['total_equity_to_assets_decile']\n",
    "temp['size_x_profitability'] = temp['ff_assets_in_usd_decile'] * temp['net_income_to_sales_decile']\n",
    "\n",
    "temp['size_x_profitability_decile'] = pd.qcut(temp['size_x_profitability'], q=100, labels=False)\n",
    "temp['size_x_leverage_decile'] = pd.qcut(temp['size_x_leverage'], q=100, labels=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "temp.groupby(by='size_x_profitability_decile')['default_1'].mean().plot(kind='bar', ax=ax[0])\n",
    "temp.groupby(by='size_x_leverage_decile')['default_1'].mean().plot(kind='bar', ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['fsym_id', 'fiscal_end_date', 'default_1', 'default_2', 'default_3', 'default_4', 'default_5', 'bankruptcy_date', 'ff_assets_in_usd', 'total_equity_to_assets', 'net_income_to_sales']].copy()\n",
    "\n",
    "# fill forward where missing (we didn't download quarterly assets in usd)\n",
    "temp['ff_assets_in_usd'] = temp.groupby('fsym_id')['ff_assets_in_usd'].ffill(limit=4)\n",
    "\n",
    "# convert to decile\n",
    "temp['ff_assets_in_usd_decile'] = pd.qcut(temp['ff_assets_in_usd'], q=10, labels=False)\n",
    "temp['total_equity_to_assets_decile'] = pd.qcut(temp['total_equity_to_assets'], q=10, labels=False)\n",
    "temp['net_income_to_sales_decile'] = pd.qcut(temp['net_income_to_sales'], q=10, labels=False)\n",
    "for m in ['ff_assets_in_usd_decile', 'total_equity_to_assets_decile', 'net_income_to_sales_decile']:\n",
    "    temp[m] = temp[m] + 1\n",
    "\n",
    "collection = []\n",
    "for i in range(1,11):\n",
    "    for j in range(1,11):\n",
    "\n",
    "        temp2 = temp[temp['ff_assets_in_usd_decile'] == i]\n",
    "        temp2 = temp2[temp2['net_income_to_sales_decile'] == j]\n",
    "        temp2 = temp2[temp2['default_1'].notnull()]\n",
    "        temp2 = temp2[temp2['default_1'] != -1] \n",
    "        \n",
    "        mean1 = temp2['default_1'].mean()\n",
    "        mean2 = temp2['default_2'].mean()\n",
    "        mean3 = temp2['default_3'].mean()\n",
    "        mean4 = temp2['default_4'].mean()\n",
    "        mean5 = temp2['default_5'].mean()\n",
    "        count = temp2.shape[0]\n",
    "\n",
    "        collection.append([i, j, mean1, mean2, mean3, mean4, mean5, count])\n",
    "\n",
    "temp = pd.DataFrame(collection, columns=['ff_assets_in_usd_decile', 'net_income_to_sales_decile', 'default_1', 'default_2', 'default_3', 'default_4', 'default_5', 'count'])\n",
    "temp = temp.pivot(index='net_income_to_sales_decile', columns='ff_assets_in_usd_decile', values=['default_1'])\n",
    "\n",
    "\n",
    "# add heatmap to table\n",
    "# temp.style.background_gradient(cmap='RdYlGn')\n",
    "temp.to_csv('/Users/joeybortfeld/Downloads/size_x_profitability_default_rates.csv', index=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[['ff_assets_in_usd', 'total_equity_to_assets', 'ff_assets_in_usd_decile', 'total_equity_to_assets_decile', 'size_x_leverage_decile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "this_var = 'net_income_to_sales'\n",
    "temp = df[df[this_var].notnull()]\n",
    "\n",
    "values_dict = {}\n",
    "for sector in temp['factset_econ_sector'].unique():\n",
    "    values_dict[sector] = temp[temp['factset_econ_sector'] == sector][this_var].values\n",
    "values_dict['All'] = temp[this_var].values\n",
    "\n",
    "\n",
    "# Run Mood's median test\n",
    "stat, p, med, table = scipy.stats.median_test(values_dict['Consumer Services'], values_dict['Distribution Services'])\n",
    "print(f\"Mood's Median Test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Reject the null hypothesis: medians are different across sectors.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: medians are not significantly different.\")\n",
    " \n",
    "\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch: bootstraping simulations to get confidence intervals\n",
    "temp = df[['fsym_id', 'fiscal_end_date', 'factset_econ_sector', 'total_equity_to_assets']].copy()\n",
    "temp = temp[temp['total_equity_to_assets'].notnull()]\n",
    "temp = temp[temp['total_equity_to_assets'] != np.inf]\n",
    "temp = temp[temp['total_equity_to_assets'] != -np.inf]\n",
    "temp = temp.sort_values(by='total_equity_to_assets', ascending=False)\n",
    "temp = temp.reset_index(drop=True)\n",
    "print(temp.shape[0])\n",
    "\n",
    "print('original median:', temp['total_equity_to_assets'].median())\n",
    "print('quintiles': )\n",
    "print()\n",
    "print('original mean:', temp['total_equity_to_assets'].mean())\n",
    "print('original sd:', temp['total_equity_to_assets'].std())\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # number of simulations\n",
    "# medians_list = []\n",
    "# for _ in tqdm.tqdm(range(10_000)):\n",
    "\n",
    "#     # resample the data (N=100_000)\n",
    "#     temp2 = temp.sample(n=200_000, replace=True)\n",
    "\n",
    "#     # calculate the median of the resampled data\n",
    "#     medians_list.append(temp2['total_equity_to_assets'].median())\n",
    "\n",
    "print('bootstrapped median:', np.median(medians_list))\n",
    "print('bootstrap median absolute deviation:', scipy.stats.median_abs_deviation(medians_list))\n",
    "print('approx 95% confidence interval:', np.percentile(medians_list, [2.5, 97.5]))\n",
    "\n",
    "# plot the histogram of the medians\n",
    "plt.hist(medians_list, bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = f'''SELECT * FROM ratio_histogram_summary_table;'''\n",
    "temp = pd.read_sql_query(query, engine)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_count_by_two_groups(data:pd.DataFrame, groupby1:str, groupby2:str, pct:bool=False):\n",
    "\n",
    "    '''\n",
    "    Generate a table of counts by two groups. Groupby1 are rows, groupby2 are columns\n",
    "    '''\n",
    "    if pct:\n",
    "        return data.groupby([groupby1, groupby2]).size().unstack() / data.groupby(groupby2).size()\n",
    "    else:\n",
    "        return data.groupby([groupby1, groupby2]).size().unstack()\n",
    "\n",
    "def obs_count_by_group(data:pd.DataFrame, groupby:str, pct:bool=False):\n",
    "    if pct:\n",
    "        return data.groupby(groupby).size() / data.shape[0]\n",
    "    else:\n",
    "        return data.groupby(groupby).size()\n",
    "\n",
    "temp  = obs_count_by_two_groups(df, 'factset_econ_sector', 'fiscal_year', pct=True)\n",
    "# obs_count_by_group(df, 'factset_econ_sector', pct=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('fiscal_year').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch: sampling\n",
    "this_rand = np.random.uniform(0, 1)\n",
    "results = []\n",
    "for _ in range(10_000):\n",
    "    temp = np.random.binomial(n=1, p=this_rand, size=100)\n",
    "    results.append(temp.mean())\n",
    "\n",
    "plt.hist(results, bins=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(this_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# s3 connection\n",
    "\n",
    "\n",
    "\n",
    "sql_connection_string = f\"postgresql+psycopg2://{aws_rds_user}:{aws_rds_password}@modeling-dataset.ci6paxfsercw.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    "\n",
    "sqlalchemy_engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{aws_rds_user}:{aws_rds_password}@modeling-dataset.ci6paxfsercw.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    ")\n",
    "\n",
    "query = f'''SELECT fsym_id, fiscal_end_date, net_debt_to_ebitda FROM modeling_dataset '''\n",
    "start = time.time()\n",
    "df3 = pd.read_sql_query(query, sqlalchemy_engine)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
