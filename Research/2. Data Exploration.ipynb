{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import scipy.stats\n",
    "from Py_Files import metric_inventory\n",
    "from Py_Files import aws_rds\n",
    "from Py_Files import credentials\n",
    "from Py_Files import data_exploration\n",
    "\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "data_dir = '/Users/joeybortfeld/Documents/QML Solutions Data/'\n",
    "s3_dir = 's3://qml-solutions-new-york/'\n",
    "metric_list = metric_inventory.ratio_dict['leverage'] + metric_inventory.ratio_dict['coverage'] + metric_inventory.ratio_dict['profitability'] + metric_inventory.ratio_dict['liquidity'] + metric_inventory.ratio_dict['volatility']\n",
    "print('ratio count:', len(metric_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. get fundamental data\n",
    "\n",
    "df = pd.read_csv(data_dir + 'qml_modeling_data/fundamental_dataset_20250109.csv')\n",
    "print('data shape:', df.shape)\n",
    "print('unique fsym_id count:', df['fsym_id'].nunique())\n",
    "print('first fiscal_end_date:', df['fiscal_end_date'].min())\n",
    "print('last fiscal_end_date:', df['fiscal_end_date'].max())\n",
    "\n",
    "# 1. get coverage data\n",
    "df_coverage = pd.read_csv(data_dir + 'universe_and_traits/qml_universe_ids.csv')\n",
    "df_coverage = df_coverage[['fsym_id', 'name1', 'name2', 'factset_econ_sector', 'factset_industry', \n",
    "                       'entity_country_hq', 'exchange_country', 'p_symbol',\n",
    "                       'max_assets_in_usd', 'factset_entity_id', 'ultimate_parent_id']]\n",
    "df_coverage = df_coverage[df_coverage['fsym_id'] != '@NA']\n",
    "\n",
    "print('coverage data shape:', df_coverage.shape)\n",
    "print(df_coverage['factset_econ_sector'].value_counts())\n",
    "print()\n",
    "\n",
    "# merge company descriptive data\n",
    "df = df.merge(df_coverage, on='fsym_id', how='left')\n",
    "\n",
    "# 2. get company default data\n",
    "df_defaults = pd.read_csv(data_dir + 'universe_and_traits/bankruptcy_data.csv')\n",
    "df_defaults['bankruptcy_date'] = pd.to_datetime(df_defaults['bankruptcy_date'])\n",
    "df_defaults = df_defaults[['fsym_id', 'bankruptcy_date']]\n",
    "df_defaults = df_defaults[df_defaults['bankruptcy_date'].notnull()]\n",
    "df_defaults = df_defaults[df_defaults['fsym_id'] != '@NA']\n",
    "df_defaults = df_defaults[df_defaults['fsym_id'] != '']\n",
    "df_defaults = df_defaults[df_defaults['fsym_id'].notnull()]\n",
    "validation = df_defaults.duplicated(subset='fsym_id', keep='first').sum()\n",
    "if validation > 0:\n",
    "    print('ALERT: bankruptcy duplicates found')\n",
    "    print('bankruptcy duplicates:', validation)\n",
    "df_defaults = df_defaults.sort_values(by=['fsym_id', 'bankruptcy_date'], ascending=False)\n",
    "df_defaults = df_defaults.drop_duplicates(subset='fsym_id', keep='last')\n",
    "\n",
    "df = df.merge(df_defaults, on='fsym_id', how='left')\n",
    "\n",
    "# 3. drop financial companies (banks, insurance, finance)\n",
    "mask1 = df['factset_econ_sector'] == 'Finance'\n",
    "mask2 = df['factset_industry'] != 'Real Estate Development'\n",
    "df = df[~(mask1 & mask2)]\n",
    "\n",
    "df['fiscal_end_date'] = pd.to_datetime(df['fiscal_end_date'])\n",
    "df['fiscal_year'] = pd.to_datetime(df['fiscal_end_date']).dt.year\n",
    "\n",
    "print(df.shape)\n",
    "print('fsym_ids with bankruptcy:', df[df['bankruptcy_date'].notnull()]['fsym_id'].nunique())\n",
    "\n",
    "# label forward defaults over 1,2,3,4,5 years\n",
    "for i in [1,2,3,4,5]:\n",
    "\n",
    "    df[f'default_{i}'] = 0\n",
    "    mask1 = (df['bankruptcy_date'] - df['fiscal_end_date']).dt.days < (365*i + 365*0.5)\n",
    "    mask2 = (df['bankruptcy_date'] - df['fiscal_end_date']).dt.days >= (365*i - 365*0.5)\n",
    "    df.loc[mask1 & mask2, f'default_{i}'] = 1\n",
    "\n",
    "    # flag -1 defaults\n",
    "    mask1 = (df['bankruptcy_date'] - df['fiscal_end_date']).dt.days < (365*i - 365*0.5)\n",
    "    df.loc[mask1, f'default_{i}'] = -1\n",
    "\n",
    "df.to_csv('/Users/joeybortfeld/Downloads/modeling_dataset_with_bankruptcy_labels.csv', index=False)\n",
    "print('done all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(s3_dir + f'qml-dashboard-tools/modeling-data/modeling_dataset_with_bankruptcy_labels_20250109.csv', index=False, storage_options=credentials.aws_s3_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Generate Quantile Distribution for Box Plots and Table Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "\n",
    "# build quantile summaries for each ratio across all sectors\n",
    "quantile_list = [0, 0.01, .02, .03, .04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95,0.96,0.97,0.98, 0.99, 1]\n",
    "groupby = 'factset_econ_sector'\n",
    "start = time.time()\n",
    "\n",
    "for m in tqdm.tqdm(metric_list):\n",
    "    \n",
    "    temp = data_exploration.quantile_analysis(df, metric=m, quantile_list=quantile_list, groupby=groupby)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_quantile_summariesquantile_summary_table_{m}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-quantile-summaries/quantile_summary_table_{m}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "print('done in', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Quantile Distribution for Box Plots to Compare Bankruptcy vs Non-Bankruptcy\n",
    "* This generates the box data (25th, 50th, 75th percentiles and more) for observations conditional that they go into bankruptcy 1,2,3,4,5 years out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "\n",
    "sector_groupby = 'factset_econ_sector'\n",
    "collection = []\n",
    "start = time.time()\n",
    "\n",
    "for this_metric in tqdm.tqdm(metric_list):\n",
    "    temp = data_exploration.quantile_analysis_by_default_class(df, this_metric, sector_groupby)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_quantile_summaries_by_default_class/quantile_summary_table_{this_metric}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-quantile-summaries-by-default-class/quantile_summary_table_{this_metric}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "\n",
    "print('done in', time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate Realized Default Rates by Ratio Deciles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "groupby = 'factset_econ_sector'\n",
    "start = time.time()\n",
    "\n",
    "for this_metric in tqdm.tqdm(metric_list):\n",
    "    temp = data_exploration.default_rate_by_ratio_decile(data=df, metric=this_metric, groupby=groupby)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_default_rates_by_decile/decile_default_rate_{this_metric}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-default-rates-by-decile/decile_default_rate_{this_metric}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "print('done in', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Histogram Data for Ratio Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build histogram\n",
    "write_to_local = True\n",
    "write_to_s3 = True\n",
    "write_to_rds = True\n",
    "groupby = 'factset_econ_sector'\n",
    "quantile_list = [0, 0.01, .02, .03, .04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95,0.96,0.97,0.98, 0.99, 1]\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "collection = []\n",
    "for this_metric in tqdm.tqdm(metric_list):\n",
    "    temp = data_exploration.generate_histogram_data(df, this_metric, quantiles=(.01, .99), groupby=groupby)\n",
    "    collection.append(temp)\n",
    "\n",
    "    if write_to_local:\n",
    "        temp.to_csv(data_dir + f'exploratory_data/ratio_histograms/ratio_histogram_summary_table_{this_metric}.csv', index=False)\n",
    "\n",
    "    if write_to_s3:\n",
    "        temp.to_csv(s3_dir + f'qml-dashboard-tools/exploratory-data/ratio-histograms/ratio_histogram_summary_table_{this_metric}.csv', index=False, storage_options=credentials.aws_s3_credentials)\n",
    "\n",
    "\n",
    "if write_to_rds:\n",
    "    print('writing to rds')\n",
    "    collection = pd.concat(collection, axis=0)\n",
    "\n",
    "    sqlalchemy_engine = aws_rds.sqlalchemy_connect_to_rds(credentials.aws_rds_credentials)\n",
    "    collection.to_sql('ratio_histogram_summary_table', sqlalchemy_engine, if_exists='replace', index=False)\n",
    "    print('done in ', time.time() - start)\n",
    "\n",
    "    # set indices in postgres database table\n",
    "    psycopg2_connection = aws_rds.psycopg2_connect_to_rds(credentials.aws_rds_credentials)\n",
    "    aws_rds.create_index_on_rds(table_name='ratio_histogram_summary_table', \n",
    "                            index_name='idx_metric_sector_lower_clip', \n",
    "                            columns_to_index=['metric', 'sector', 'lower_clip'], \n",
    "                            conn=psycopg2_connection)\n",
    "\n",
    "\n",
    "        \n",
    "print('done in ', time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('writing to rds')\n",
    "collection = pd.concat(collection, axis=0)\n",
    "\n",
    "sqlalchemy_engine = aws_rds.sqlalchemy_connect_to_rds(credentials.aws_rds_credentials)\n",
    "collection.to_sql('ratio_histogram_summary_table', sqlalchemy_engine, if_exists='replace', index=False)\n",
    "print('done in ', time.time() - start)\n",
    "\n",
    "# set indices in postgres database table\n",
    "psycopg2_connection = aws_rds.psycopg2_connect_to_rds(credentials.aws_rds_credentials)\n",
    "aws_rds.create_index_on_rds(table_name='ratio_histogram_summary_table', \n",
    "                        index_name='idx_metric_sector_lower_clip', \n",
    "                        columns_to_index=['metric', 'sector', 'lower_clip'], \n",
    "                        conn=psycopg2_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bankruptcy Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = data_exploration.build_default_diagnostics(df)\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    print(f'{i}Y defaults with assets/ebitda/cf:', (temp[f'fund_count_{i}'] == 3).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['fsym_id', 'fiscal_end_date', 'bankruptcy_date', 'default_1', 'total_debt_to_ebitda', 'total_debt', 'ff_debt_st', 'ff_debt_lt', 'ff_ebitda_oper_ltm']].copy()\n",
    "temp = temp[temp['total_debt_to_ebitda'].notnull()]\n",
    "temp['decile'] = pd.qcut(temp['total_debt_to_ebitda'], q=10, labels=False, duplicates='drop')\n",
    "temp = temp[temp['default_1'] != -1]\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "temp.groupby('decile')['default_1'].mean().plot(kind='bar', ax=ax[0], title='Debt-to-EBITDA')\n",
    "\n",
    "# try inverse\n",
    "# temp['ebitda_to_total_debt'] = 1 / temp['total_debt_to_ebitda']\n",
    "temp['ebitda_to_total_debt'] = temp['ff_ebitda_oper_ltm'] / temp['total_debt'].clip(lower=0.01)\n",
    "temp['decile'] = pd.qcut(temp['ebitda_to_total_debt'], q=10, labels=False, duplicates='drop')\n",
    "temp.groupby('decile')['default_1'].mean().plot(kind='bar', ax=ax[1], title='EBITDA to Total Debt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(data_dir + f'exploratory_data/ratio_default_rates_by_decile/decile_default_rate_total_equity_to_assets.csv')\n",
    "temp = temp.head(10)\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "rho, p =scipy.stats.spearmanr(temp['decile'], temp['default_rate'])\n",
    "rho = rho.round(2)\n",
    "p = '{:.3f}'.format(p)\n",
    "rho,p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_var = 'total_equity_to_assets'\n",
    "\n",
    "temp = df[df[this_var].notnull()].copy()\n",
    "temp = temp[temp[this_var] != np.inf]\n",
    "temp = temp[temp[this_var] != -np.inf]\n",
    "temp['constant'] = 1\n",
    "\n",
    "lower, upper = temp[this_var].quantile([0.01, 0.99])\n",
    "temp[this_var] = temp[this_var].clip(lower=lower, upper=upper)\n",
    "\n",
    "\n",
    "# logitit regression\n",
    "y = temp['default_1']\n",
    "X = temp[[this_var, 'constant']]\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import sklearn.metrics\n",
    "\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# calculate auROC\n",
    "predictions = result.predict(X)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y, predictions)\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "result.summary()    \n",
    "print('AUROC:', roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(data_dir + f'universe_and_traits/qml_universe_ids.csv')\n",
    "\n",
    "temp = temp[temp['entity_country_hq']=='United States']\n",
    "temp = temp[temp['max_assets_in_usd'] > 1_000]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "this_var = 'net_income_to_sales'\n",
    "temp = df[df[this_var].notnull()]\n",
    "\n",
    "values_dict = {}\n",
    "for sector in temp['factset_econ_sector'].unique():\n",
    "    values_dict[sector] = temp[temp['factset_econ_sector'] == sector][this_var].values\n",
    "values_dict['All'] = temp[this_var].values\n",
    "\n",
    "\n",
    "# Run Mood's median test\n",
    "stat, p, med, table = scipy.stats.median_test(values_dict['Consumer Services'], values_dict['Distribution Services'])\n",
    "print(f\"Mood's Median Test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Reject the null hypothesis: medians are different across sectors.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: medians are not significantly different.\")\n",
    " \n",
    "\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch: bootstraping simulations to get confidence intervals\n",
    "temp = df[['fsym_id', 'fiscal_end_date', 'factset_econ_sector', 'total_equity_to_assets']].copy()\n",
    "temp = temp[temp['total_equity_to_assets'].notnull()]\n",
    "temp = temp[temp['total_equity_to_assets'] != np.inf]\n",
    "temp = temp[temp['total_equity_to_assets'] != -np.inf]\n",
    "temp = temp.sort_values(by='total_equity_to_assets', ascending=False)\n",
    "temp = temp.reset_index(drop=True)\n",
    "print(temp.shape[0])\n",
    "\n",
    "print('original median:', temp['total_equity_to_assets'].median())\n",
    "print('quintiles': )\n",
    "print()\n",
    "print('original mean:', temp['total_equity_to_assets'].mean())\n",
    "print('original sd:', temp['total_equity_to_assets'].std())\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # number of simulations\n",
    "# medians_list = []\n",
    "# for _ in tqdm.tqdm(range(10_000)):\n",
    "\n",
    "#     # resample the data (N=100_000)\n",
    "#     temp2 = temp.sample(n=200_000, replace=True)\n",
    "\n",
    "#     # calculate the median of the resampled data\n",
    "#     medians_list.append(temp2['total_equity_to_assets'].median())\n",
    "\n",
    "print('bootstrapped median:', np.median(medians_list))\n",
    "print('bootstrap median absolute deviation:', scipy.stats.median_abs_deviation(medians_list))\n",
    "print('approx 95% confidence interval:', np.percentile(medians_list, [2.5, 97.5]))\n",
    "\n",
    "# plot the histogram of the medians\n",
    "plt.hist(medians_list, bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = f'''SELECT * FROM ratio_histogram_summary_table;'''\n",
    "temp = pd.read_sql_query(query, engine)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_count_by_two_groups(data:pd.DataFrame, groupby1:str, groupby2:str, pct:bool=False):\n",
    "\n",
    "    '''\n",
    "    Generate a table of counts by two groups. Groupby1 are rows, groupby2 are columns\n",
    "    '''\n",
    "    if pct:\n",
    "        return data.groupby([groupby1, groupby2]).size().unstack() / data.groupby(groupby2).size()\n",
    "    else:\n",
    "        return data.groupby([groupby1, groupby2]).size().unstack()\n",
    "\n",
    "def obs_count_by_group(data:pd.DataFrame, groupby:str, pct:bool=False):\n",
    "    if pct:\n",
    "        return data.groupby(groupby).size() / data.shape[0]\n",
    "    else:\n",
    "        return data.groupby(groupby).size()\n",
    "\n",
    "temp  = obs_count_by_two_groups(df, 'factset_econ_sector', 'fiscal_year', pct=True)\n",
    "# obs_count_by_group(df, 'factset_econ_sector', pct=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('fiscal_year').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch: sampling\n",
    "this_rand = np.random.uniform(0, 1)\n",
    "results = []\n",
    "for _ in range(10_000):\n",
    "    temp = np.random.binomial(n=1, p=this_rand, size=100)\n",
    "    results.append(temp.mean())\n",
    "\n",
    "plt.hist(results, bins=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(this_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# s3 connection\n",
    "\n",
    "\n",
    "\n",
    "sql_connection_string = f\"postgresql+psycopg2://{aws_rds_user}:{aws_rds_password}@modeling-dataset.ci6paxfsercw.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    "\n",
    "sqlalchemy_engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{aws_rds_user}:{aws_rds_password}@modeling-dataset.ci6paxfsercw.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    ")\n",
    "\n",
    "query = f'''SELECT fsym_id, fiscal_end_date, net_debt_to_ebitda FROM modeling_dataset '''\n",
    "start = time.time()\n",
    "df3 = pd.read_sql_query(query, sqlalchemy_engine)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
