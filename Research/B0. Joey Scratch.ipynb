{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import tqdm\n",
    "import os\n",
    "from Py_Files import credentials\n",
    "from Py_Files import factset_api\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "for this_index in ['C0A0', 'H0A0', 'ER00', 'HE00']:\n",
    "\n",
    "    temp = pd.read_excel(f'/Users/joeybortfeld/Documents/QML Solutions Data/ice/ICE Jan 15/{this_index}-01152025.xlsx', skiprows=1)\n",
    "    temp = temp[temp['Description'] != '']\n",
    "    temp = temp[temp['Description'].notnull()]\n",
    "\n",
    "    # rename columns to camel case with underscore\n",
    "    temp.columns = [col.lower().replace(' ', '_') for col in temp.columns]\n",
    "    temp.columns = [col.replace('.', '') for col in temp.columns]\n",
    "    temp.columns = [col.replace('_(sa)', '') for col in temp.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp['index'] = this_index\n",
    "    temp = temp[['cusip', 'isin_number', 'description', 'ticker', 'index', 'par_wtd_coupon', 'maturity_date', 'rating', 'iso_currency_code', 'iso_country_code', \n",
    "                 'industry_lvl_2_desc', 'industry_lvl_3_desc', 'industry_lvl_4_desc', \n",
    "                 'face_value_loc', 'price', 'oas', 'subordination_type', 'mod_dur_to_worst']]\n",
    "\n",
    "    collection.append(temp)\n",
    "\n",
    "df = pd.concat(collection, axis=0)\n",
    "df['constant'] = 1\n",
    "\n",
    "# rating preprocessing\n",
    "rating_to_num_dict = {'AAA': 21, 'AA1': 20, 'AA2': 19, 'AA3': 18, \n",
    "                      'A1': 17, 'A2': 16, 'A3': 15, \n",
    "                      'BBB1': 14, 'BBB2': 13, 'BBB3': 12, \n",
    "                      'BB1': 11, 'BB2': 10, 'BB3': 9, \n",
    "                      'B1': 8, 'B2': 7, 'B3': 6, \n",
    "                      'CCC1': 5, 'CCC2': 4, 'CCC3': 3, \n",
    "                      'CC': 2, 'C': 1, 'D': 0}\n",
    "num_to_rating_dict = {v: k for k, v in rating_to_num_dict.items()}\n",
    "df['rating_num'] = df['rating'].map(lambda x: rating_to_num_dict[x])\n",
    "\n",
    "rating_to_broad_rating_dict = {'AAA': '>=AA', 'AA1': '>=AA', 'AA2': '>=AA', 'AA3': '>=AA', \n",
    "                      'A1': 'A', 'A2': 'A', 'A3': 'A', \n",
    "                      'BBB1': 'BBB', 'BBB2': 'BBB', 'BBB3': 'BBB', \n",
    "                      'BB1': 'BB', 'BB2': 'BB', 'BB3': 'BB', \n",
    "                      'B1': 'B', 'B2': 'B', 'B3': 'B', \n",
    "                      'CCC1': '<=CCC', 'CCC2': '<=CCC', 'CCC3': '<=CCC', \n",
    "                      'CC': '<=CCC', 'C': '<=CCC', 'D': '<=CCC'}\n",
    "df['broad_rating'] = df['rating'].map(lambda x: rating_to_broad_rating_dict[x])\n",
    "\n",
    "broad_rating_to_num_dict = {'>=AA': 6, 'A': 5, 'BBB': 4, 'BB': 3, 'B': 2, '<=CCC': 1,}\n",
    "df['broad_rating_num'] = df['broad_rating'].map(lambda x: broad_rating_to_num_dict[x])\n",
    "\n",
    "# calculate face value in USD\n",
    "exchange_rate_to_usd_dict = {'USD': 1.0, 'EUR': 1.03} \n",
    "df['exchange_rate_to_usd'] = df['iso_currency_code'].map(lambda x: exchange_rate_to_usd_dict[x])\n",
    "df['face_value_in_usd'] = df['face_value_loc'] * df['exchange_rate_to_usd']\n",
    "\n",
    "# descriptive statistics\n",
    "print('ICE indices:', df['index'].unique())\n",
    "print('unique ticker count:', df['ticker'].nunique())\n",
    "print('unique description count:', df['description'].nunique())\n",
    "print('min maturity date:', df['maturity_date'].min())\n",
    "print('max maturity date:', df['maturity_date'].max())\n",
    "print('min oas:', df['oas'].min())\n",
    "print('max oas:', df['oas'].max())\n",
    "print('unique rating count:', df['rating'].nunique())\n",
    "print('unique subordination type count:', df['subordination_type'].nunique())\n",
    "print('unique industry lvl 2 desc count:', df['industry_lvl_2_desc'].nunique())\n",
    "print('unique industry lvl 3 desc count:', df['industry_lvl_3_desc'].nunique())\n",
    "print('unique industry lvl 4 desc count:', df['industry_lvl_4_desc'].nunique())\n",
    "print('unique iso country code count:', df['iso_country_code'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAML data exploration\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(1234)\n",
    "lower_ci_bound = 2.5\n",
    "upper_ci_bound = 97.5\n",
    "\n",
    "lower_oas_bound = .05\n",
    "upper_oas_bound = .95\n",
    "\n",
    "lower_dur_bound = 4.0\n",
    "upper_dur_bound = 6.0\n",
    "\n",
    "\n",
    "\n",
    "region_dict = {'USD': ['C0A0', 'H0A0'], 'EUR': ['ER00', 'HE00'], 'All': ['C0A0', 'H0A0', 'ER00', 'HE00']}\n",
    "\n",
    "# median OAS by rating\n",
    "rating_var = 'broad_rating_num'\n",
    "\n",
    "collection = []\n",
    "for this_region in ['USD', 'EUR', 'All']:\n",
    "\n",
    "    # subset for the specified region\n",
    "    dff = df[df['index'].isin(region_dict[this_region])].copy()\n",
    "    dff = dff[dff['mod_dur_to_worst'] >= lower_dur_bound]\n",
    "    dff = dff[dff['mod_dur_to_worst'] <= upper_dur_bound]\n",
    "\n",
    "    # iterate over each distinct rating\n",
    "    for this_rating in dff[rating_var].unique():\n",
    "        \n",
    "        df1 = dff[dff[rating_var] == this_rating].copy()\n",
    "\n",
    "        # bond count\n",
    "        bond_count = len(df1)\n",
    "\n",
    "        # calculate the median oas\n",
    "        median_oas = df1['oas'].median()\n",
    "\n",
    "        # bootstrap to get a confidence interval of the median\n",
    "        sample_list = []\n",
    "        for _ in range(1_000):\n",
    "            sample = df1['oas'].sample(n=len(df1), replace=True)\n",
    "            sample_list.append(sample.median())\n",
    "        lower_est = np.percentile(sample_list, lower_ci_bound)\n",
    "        upper_est = np.percentile(sample_list, upper_ci_bound)\n",
    "\n",
    "        # calculate the range of the observed data\n",
    "        lower_oas = df1['oas'].quantile(lower_oas_bound)\n",
    "        upper_oas = df1['oas'].quantile(upper_oas_bound)\n",
    "\n",
    "        # append to collection\n",
    "        collection.append([this_region, this_rating, bond_count, lower_oas, lower_est, median_oas, upper_est, upper_oas])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "collection = pd.DataFrame(collection, columns=['region', 'rating', 'bond_count', 'lower_oas', 'lower_est', 'median_oas', 'upper_est', 'upper_oas'])\n",
    "collection = collection.sort_values(by=['region', 'rating'])\n",
    "collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_est, median_oas, upper_est, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baml_industry_aggregations(data:pd.DataFrame, industry='industry_lvl_3_desc'):\n",
    "\n",
    "    groupby = ['description', industry]\n",
    "    df = data.groupby(groupby, as_index=False)['face_value_in_usd'].sum()\n",
    "    df = df.sort_values(by=groupby, ascending=[True,True])\n",
    "    df = df.drop_duplicates(subset=groupby, keep='last')\n",
    "\n",
    "    df = df[['description', industry]]\n",
    "    df.columns = ['description', f'majority_{industry}']\n",
    "\n",
    "    return df\n",
    "\n",
    "description_industry_lvl_3 = baml_industry_aggregations(df, industry='industry_lvl_3_desc')\n",
    "description_industry_lvl_4 = baml_industry_aggregations(df, industry='industry_lvl_4_desc')\n",
    "description_industry_lvl_3_4 = description_industry_lvl_3.merge(description_industry_lvl_4, on='description', how='left')\n",
    "description_industry_lvl_3_4.to_csv('/Users/joeybortfeld/Downloads/description_industry_lvl_3_4.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baml_oas_aggregations(data:pd.DataFrame, lower_duration:float=4.0, upper_duration:float=6.0):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    df = df[df['mod_dur_to_worst'] >= lower_duration]\n",
    "    df = df[df['mod_dur_to_worst'] <= upper_duration]\n",
    "\n",
    "    df = df.groupby(by=['description', 'ticker', 'rating'])['oas'].median().reset_index(drop=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "temp = baml_oas_aggregations(df)\n",
    "temp.to_csv('/Users/joeybortfeld/Downloads/baml_oas_aggregations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DESCRIPTION-RATING-TYPE AGGREGATIONS\n",
    "\n",
    "def baml_rating_aggregations(data:pd.DataFrame):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # 0. generate aggregations to produce a dataset where rows are unique combinations of ['description', 'subordination_type', 'rating']\n",
    "    # aggregations by ['description', 'subordination_type', 'rating']\n",
    "    groupby = ['description', 'ticker', 'subordination_type', 'rating']\n",
    "    df['bond_count'] = df.groupby(groupby)['constant'].transform('sum')\n",
    "    df['total_face_value_in_usd'] = df.groupby(groupby)['face_value_in_usd'].transform('sum')\n",
    "\n",
    "    df = df.drop_duplicates(subset=groupby, keep='first')\n",
    "    df = df.sort_values(by=groupby)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # generate a tuple with all the ratings info (subordination_type, rating, bond_count, total_face_value_in_usd) for each description\n",
    "    df['all_rating_info'] = df[['subordination_type', 'rating', 'ticker', 'bond_count', 'total_face_value_in_usd']].apply(lambda x: f'({x[0]}, {x[1]}, {x[2]}, {int(x[3])})', axis=1)\n",
    "    rating_summary_tuple = df.groupby(by=['description', 'ticker'])['all_rating_info'].apply(lambda x: list(set(x)))\n",
    "    rating_summary_tuple = rating_summary_tuple.reset_index(drop=False)\n",
    "\n",
    "    df = df.drop(['all_rating_info'], axis=1)\n",
    "    df = df.merge(rating_summary_tuple, on=['description', 'ticker'], how='left')\n",
    "\n",
    "    df['rating_type_count'] = df.groupby(by=['description', 'ticker'])['constant'].transform('sum')\n",
    "\n",
    "    # generate a single rating for each subordination type\n",
    "    for this_type in ['Senior', 'Secured', 'Subordinated', 'Senior Non Preferred', 'Tier 2', 'Alternative Tier 1', 'Tier 1', 'Junior subordinated', 'Preferred', 'Upper tier 2']:\n",
    "\n",
    "        label = this_type.lower().replace(' ','_')\n",
    "        \n",
    "        dff = df[df['subordination_type'] == this_type].copy()\n",
    "        \n",
    "        # flag if split rating\n",
    "        dff['max_rating_num'] = dff.groupby(by=['description', 'ticker'])['rating_num'].transform('max')\n",
    "        dff['min_rating_num'] = dff.groupby(by=['description', 'ticker'])['rating_num'].transform('min')\n",
    "        dff[f'{label}_split_rating'] = dff['max_rating_num'] - dff['min_rating_num']\n",
    "\n",
    "        # generate a single rating for each subordination type as the minimum rating\n",
    "        dff[f'{label}_rating_num'] = dff.groupby(by=['description', 'ticker'])['rating_num'].transform('min')\n",
    "\n",
    "        dff[f'{label}_rating'] = dff[f'{label}_rating_num'].map(num_to_rating_dict)\n",
    "        dff = dff[['description', 'ticker', f'{label}_rating', f'{label}_split_rating']]\n",
    "\n",
    "        df = df.merge(dff, on=['description', 'ticker'], how='left')\n",
    "\n",
    "        df[f'{label}_rating'] = df[f'{label}_rating'].fillna('NA')\n",
    "\n",
    "    # generate a single representative rating for each description\n",
    "    df['single_rating'] = ''\n",
    "    df['single_rating_type'] = ''\n",
    "    df['single_split_rating'] = np.NaN\n",
    "    for this_type in ['senior', 'senior_non_preferred', 'tier_1', 'tier_2', 'secured', 'subordinated', 'junior_subordinated',]:\n",
    "        mask1 = df['single_rating'] == ''\n",
    "        mask2 = df[f'{this_type}_rating'] != 'NA'\n",
    "\n",
    "        df.loc[mask1 & mask2, 'single_rating'] = df.loc[mask1 & mask2, f'{this_type}_rating']\n",
    "        df.loc[mask1 & mask2, 'single_rating_type'] = this_type\n",
    "        df.loc[mask1 & mask2, 'single_split_rating'] = df.loc[mask1 & mask2, f'{this_type}_split_rating']\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['description', 'ticker'], keep='first')\n",
    "    return df[['description', 'ticker', \n",
    "               'bond_count', 'total_face_value_in_usd', 'all_rating_info', 'rating_type_count',\n",
    "               'single_rating', 'single_rating_type', 'single_split_rating',\n",
    "               'senior_rating', 'senior_split_rating', \n",
    "               'secured_rating', 'secured_split_rating', \n",
    "               'subordinated_rating', 'subordinated_split_rating',\n",
    "               'senior_non_preferred_rating', 'senior_non_preferred_split_rating',\n",
    "                'tier_2_rating', 'tier_2_split_rating', \n",
    "                'alternative_tier_1_rating', 'alternative_tier_1_split_rating',\n",
    "                'tier_1_rating', 'tier_1_split_rating',\n",
    "                'junior_subordinated_rating', 'junior_subordinated_split_rating',\n",
    "                'preferred_rating', 'preferred_split_rating',\n",
    "                'upper_tier_2_rating', 'upper_tier_2_split_rating']]\n",
    "\n",
    "\n",
    "temp = baml_rating_aggregations(df)\n",
    "temp['single_rating_bucket'] = temp['single_rating'].map(rating_to_broad_rating_dict)\n",
    "temp =temp[['description', 'ticker', 'single_rating', 'single_rating_bucket', 'total_face_value_in_usd']]\n",
    "temp = temp.merge(description_industry_lvl_3_4, on='description', how='left')\n",
    "\n",
    "temp = temp[temp['majority_industry_lvl_3_desc'] != 'Financial Services']\n",
    "temp = temp[temp['majority_industry_lvl_3_desc'] != 'Banking']\n",
    "temp = temp[temp['majority_industry_lvl_3_desc'] != 'Insurance']\n",
    "\n",
    "temp = temp.sort_values(by=['single_rating_bucket', 'total_face_value_in_usd'], ascending=[True, False])\n",
    "temp = temp.drop_duplicates(subset=['description'], keep='first')\n",
    "temp = temp.groupby(by='single_rating_bucket').head(70)\n",
    "temp.to_csv('/Users/joeybortfeld/Downloads/largest_issuers_by_rating_bucke3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['rating_num'] >= 5]\n",
    "temp = df\n",
    "temp = temp[temp['mod_dur_to_worst'] >= 5.0]\n",
    "temp = temp[temp['mod_dur_to_worst'] <= 9.0]\n",
    "temp.groupby('broad_rating_num')['oas'].median().apply(lambda x: np.log(x)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/joeybortfeld/Downloads/largest_issuers_by_rating_bucket_w_fsym_id.csv', skiprows=2, encoding='latin1')\n",
    "\n",
    "# build rating fields\n",
    "df['single_rating_num'] = df['single_rating'].map(rating_to_num_dict)\n",
    "df['single_rating_bucket'] = df['single_rating'].map(rating_to_broad_rating_dict)\n",
    "df['single_rating_bucket_num'] = df['single_rating_bucket'].map(broad_rating_to_num_dict)\n",
    "\n",
    "# filter for issuers with fsym_id\n",
    "df = df[df['fsym_id'].notnull()]\n",
    "df = df[df['fsym_id'] != 'NA']\n",
    "\n",
    "temp = pd.read_csv('/Users/joeybortfeld/Downloads/most_recent_pd_by_fsym.csv')\n",
    "df = df.merge(temp, on='fsym_id', how='left')\n",
    "\n",
    "df[['description', 'fsym_id', 'majority_industry_lvl_3_desc', 'single_rating', 'single_rating_bucket', 'oas_4_6', 'cumulative_pd_5']]\n",
    "\n",
    "df['cumulative_pd_5'] = df['cumulative_pd_5'].apply(lambda x: np.log(x))\n",
    "df['oas_4_6'] = df['oas_4_6'].apply(lambda x: np.log(x))\n",
    "\n",
    "# chart\n",
    "fig = px.strip(df, x='single_rating_bucket_num', y='oas_4_6', color=\"single_rating_bucket\").update_traces(jitter = 1)\n",
    "fig = px.strip(df, x='oas_4_6', y='cumulative_pd_5',).update_traces(jitter = 1)\n",
    "fig.show()\n",
    "\n",
    "print('correlation between oas and pd:')\n",
    "print(df[['cumulative_pd_5', 'oas_4_6']].corr())\n",
    "\n",
    "# df['cumulative_pd_5'] *= (1-.40)\n",
    "# df['cumulative_pd_5'] *= 10_000\n",
    "\n",
    "# df['oas_4_6'] = df['oas_4_6'].apply(lambda x: np.log(x))\n",
    "# df['cumulative_pd_5'] = df['cumulative_pd_5'].apply(lambda x: np.log(x))\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "# df.groupby(by='single_rating_bucket_num')['cumulative_pd_5'].median().plot(label='PD', kind='bar', ax=ax, title='Median 5Y PD by Rating Bucket')\n",
    "# df.groupby(by='single_rating_bucket_num')['cumulative_pd_5'].count().plot(label='PD', kind='bar', ax=ax, secondary_y=True, title='Counts by Rating Bucket', color='red')\n",
    "# df.groupby(by='single_rating_bucket_num')['oas_4_6'].median().plot(label='oas', ax=ax, secondary_y=True, color='red')\n",
    "# ax[0].legend()\n",
    "# ax[1].legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='single_rating_bucket_num')[['cumulative_pd_5', 'oas_4_6']].median().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Py_Files import factset_api\n",
    "from Py_Files import credentials\n",
    "\n",
    "factset_api.download_company_profile(id_list=['AAPL-US',], authorization=credentials.factset_api_authorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests\n",
    "\n",
    "def get_stock_prices(id_list:str=['MH33D6-R'], \n",
    "                     field_list:list=[\"price\", \"volume\", \"tradeCount\"], \n",
    "                     start_date:str='2006-01-03', \n",
    "                     end_date:str='2024-12-31', \n",
    "                     frequency:str='D',\n",
    "                     split:str='UNSPLIT',\n",
    "                     verbose:bool=False,\n",
    "                     authorization=None):\n",
    "\n",
    "    '''\n",
    "    Get stock prices for a given ticker.\n",
    "\n",
    "    Split is either SPLIT, SPLIT_SPINOFF', UNSPLIT. For the purpose of constructing historical market capitaliation use\n",
    "    UNSPLIT to be on a like-for-like basis with shares outsanding as reported in the financial statements. \n",
    "    '''\n",
    "\n",
    "    prices_endpoint = 'https://api.factset.com/content/factset-global-prices/v1/prices'\n",
    "\n",
    "    prices_request ={\n",
    "    \"ids\": id_list,\n",
    "        \"fields\": field_list,\n",
    "        \"startDate\":start_date,\n",
    "        \"endDate\":end_date,\n",
    "        \"frequency\":frequency,\n",
    "        \"adjust\":split,\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "    #create a post request\n",
    "    prices_post = json.dumps(prices_request)\n",
    "\n",
    "    if verbose:\n",
    "        print('post request:')\n",
    "        print(prices_endpoint)\n",
    "        print(prices_post)\n",
    "        print()\n",
    "\n",
    "    prices_response = requests.post(url = prices_endpoint, data=prices_post, auth = authorization, headers = headers, verify= False )\n",
    "\n",
    "    if verbose:\n",
    "        print('HTTP Status: {}'.format(prices_response.status_code))\n",
    "        print(prices_response.text)\n",
    "\n",
    "    if prices_response.status_code != 200:\n",
    "        if verbose:\n",
    "            print('error: failed to get stock prices')\n",
    "        return [prices_response.status_code,None]\n",
    "    else:\n",
    "        prices_data = json.loads(prices_response.text)\n",
    "        prices_df = pd.DataFrame(prices_data['data'])\n",
    "        return [prices_response.status_code, prices_df]\n",
    "\n",
    "def get_stock_returns(id_list:str=['MH33D6-R'], \n",
    "                     start_date:str='2006-01-03', \n",
    "                     end_date:str='2024-12-31', \n",
    "                     frequency:str='D',\n",
    "                     verbose:bool=False,\n",
    "                     authorization=None):\n",
    "\n",
    "    '''\n",
    "    Get stock returns.\n",
    "\n",
    "    '''\n",
    "\n",
    "    returns_endpoint = 'https://api.factset.com/content/factset-global-prices/v1/returns'\n",
    "\n",
    "    returns_request ={\n",
    "    \"ids\": id_list,\n",
    "        \"startDate\":start_date,\n",
    "        \"endDate\":end_date,\n",
    "        \"frequency\":\"D\",\n",
    "        \"dividendAdjust\": \"EXDATE\"\n",
    "    }\n",
    "\n",
    "    headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "    #create a post request\n",
    "    returns_post = json.dumps(returns_request)\n",
    "\n",
    "    if verbose:\n",
    "        print('post request:')\n",
    "        print(returns_endpoint)\n",
    "        print(returns_post)\n",
    "        print()\n",
    "\n",
    "    returns_response = requests.post(url = returns_endpoint, data=returns_post, auth = authorization, headers = headers, verify= False )\n",
    "\n",
    "    if verbose:\n",
    "        print('HTTP Status: {}'.format(returns_response.status_code))\n",
    "        print(returns_response.text)\n",
    "\n",
    "    if returns_response.status_code != 200:\n",
    "        if verbose:\n",
    "            print('error: failed to get stock prices')\n",
    "        return [returns_response.status_code,None]\n",
    "    else:\n",
    "        returns_data = json.loads(returns_response.text)\n",
    "        returns_df = pd.DataFrame(returns_data['data'])\n",
    "        return [returns_response.status_code, returns_df]\n",
    "\n",
    "\n",
    "\n",
    "def get_shares_outanding(id_list:str=['MH33D6-R'], \n",
    "                     start_date:str='2006-01-03', \n",
    "                     end_date:str='2024-12-31', \n",
    "                     frequency:str='D',\n",
    "                     verbose:bool=False,\n",
    "                     authorization=None):\n",
    "\n",
    "    '''\n",
    "    Get shares outstanding for a given ID\n",
    "    '''\n",
    "\n",
    "    shares_endpoint = 'https://api.factset.com/content/factset-global-prices/v1/security-shares'\n",
    "\n",
    "    shares_request ={\n",
    "        'data': {\n",
    "            \"ids\": id_list,\n",
    "            \"startDate\":start_date,\n",
    "            \"endDate\":end_date,\n",
    "            \"frequency\":frequency,\n",
    "            \"calendar\": 'FIVEDAY',\n",
    "            \"batch\": \"N\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {'Accept': 'application/json','Content-Type': 'application/json'}\n",
    "\n",
    "    #create a post request\n",
    "    shares_post = json.dumps(shares_request)\n",
    "\n",
    "    if verbose:\n",
    "        print('post request:')\n",
    "        print(shares_endpoint)\n",
    "        print(shares_post)\n",
    "        print()\n",
    "\n",
    "    shares_response = requests.post(url=shares_endpoint, \n",
    "                                    data=shares_post, \n",
    "                                    auth=authorization, \n",
    "                                    headers=headers, \n",
    "                                    verify=False )\n",
    "\n",
    "    if verbose:\n",
    "        print('HTTP Status: {}'.format(shares_response.status_code))\n",
    "        print(shares_response.text)\n",
    "\n",
    "    if shares_response.status_code != 200:\n",
    "        if verbose:\n",
    "            print('error: failed to get shares outstanding')\n",
    "        return [shares_response.status_code,None]\n",
    "    else:\n",
    "        shares_data = json.loads(shares_response.text)\n",
    "        shares_df = pd.DataFrame(shares_data['data'])\n",
    "        return [shares_response.status_code, shares_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_get_stock_returns(id_list:list=['MH33D6-R'], \n",
    "                     start_date:str='2006-01-03', \n",
    "                     end_date:str='2024-12-31', \n",
    "                     frequency:str='D',\n",
    "                     verbose:bool=False,\n",
    "                     authorization=None):\n",
    "\n",
    "    \n",
    "\n",
    "    status, df = get_stock_returns(id_list=['DBNXVB-R'], \n",
    "                        start_date='2006-01-03', \n",
    "                        end_date='2024-12-31', \n",
    "                        frequency='D',\n",
    "                        verbose=False,\n",
    "                        authorization=credentials.factset_api_authorization)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, df_split = get_stock_prices(id_list=['DBNXVB-R'], \n",
    "                     field_list=[\"price\", \"volume\", \"tradeCount\"], \n",
    "                     start_date='2006-01-03', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='D',\n",
    "                     split='SPLIT',\n",
    "                     verbose=False,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "\n",
    "status, df_unsplit = get_stock_prices(id_list=['DBNXVB-R'], \n",
    "                     field_list=[\"price\", \"volume\", \"tradeCount\"], \n",
    "                     start_date='2006-01-03', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='D',\n",
    "                     split='SPLIT_SPINOFF',\n",
    "                     verbose=False,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "\n",
    "df = df_split[['date', 'price']].merge(df_unsplit[['date', 'price']], on='date', how='left')\n",
    "df.columns = ['date', 'price_split', 'price_unsplit']\n",
    "df.set_index('date')[['price_split', 'price_unsplit']].plot()\n",
    "\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, temp = get_shares_outanding(id_list=['DBNXVB-R'], \n",
    "                     start_date='2006-01-01', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='D',\n",
    "                     verbose=True,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "print(status)\n",
    "print(temp.columns)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_cap(id_list:str=['MH33D6-R'], \n",
    "                     start_date:str='2006-01-03', \n",
    "                     end_date:str='2024-12-31', \n",
    "                     frequency:str='M',\n",
    "                     verbose:bool=False,\n",
    "                     authorization=None):\n",
    "    \n",
    "    status1, df1 = get_stock_prices(id_list=id_list, \n",
    "                     field_list=[\"price\", \"volume\", \"tradeCount\"], \n",
    "                     start_date=start_date, \n",
    "                     end_date=end_date, \n",
    "                     frequency=frequency,\n",
    "                     split='SPLIT',\n",
    "                     verbose=verbose,\n",
    "                     authorization=authorization)\n",
    "\n",
    "    status2, df2 = get_shares_outanding(id_list=id_list, \n",
    "                     start_date=start_date, \n",
    "                     end_date=end_date, \n",
    "                     frequency=frequency,\n",
    "                     verbose=verbose,\n",
    "                     authorization=authorization)\n",
    "    \n",
    "    df = df1[['date', 'price']].merge(df2[['date', 'totalOutstanding']], on='date', how='left')\n",
    "    df['market_cap'] = df['price'] * df['totalOutstanding']*1_000_000  # shares are in millions\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_market_cap(id_list=['DBNXVB-R'], \n",
    "                    start_date='2006-01-03', \n",
    "                    end_date='2024-12-31', \n",
    "                    frequency='M',\n",
    "                    verbose=False,\n",
    "                    authorization=credentials.factset_api_authorization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date')['market_cap'].plot()\n",
    "df['market_cap'] /= 1_000_000_000\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factset_universe = pd.read_csv('/Users/joeybortfeld/Downloads/qml_universe_ids.csv')\n",
    "factset_universe = factset_universe.sort_values(by='max_assets_in_usd', ascending=False)\n",
    "\n",
    "universe_dict = factset_api.load_universe_dict(factset_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = factset_api.batch_get_stock_data(metric='prices', \n",
    "                                #   fsym_list=['DBNXVB-R', 'MH33D6-R', 'P8R3C2-R', 'NNKD2Y-R', 'K4GK55-R'], \n",
    "                                fsym_list=universe_dict['$1B'], \n",
    "                                  start_date='2006-01-03', \n",
    "                                  end_date='2024-12-31', \n",
    "                                  frequency='D',\n",
    "                                  verbose=True,\n",
    "                                  authorization=credentials.factset_api_authorization,\n",
    "                                  skip_if_done=True,\n",
    "                                  output_folder='/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/')\n",
    "\n",
    "\n",
    "temp = factset_universe[['fsym_id', 'name1', 'name2', 'factset_sector', 'factset_industry', 'max_assets_in_usd']].copy()\n",
    "temp = temp[temp['max_assets_in_usd'] > 1_000]\n",
    "\n",
    "# fails\n",
    "temp1 = pd.DataFrame(response, columns=['fsym_id'])\n",
    "temp1['status'] = 'fail'\n",
    "\n",
    "# successes\n",
    "successes = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/')]\n",
    "temp2 = pd.DataFrame(successes, columns=['fsym_id'])\n",
    "temp2['status'] = 'success'\n",
    "\n",
    "temp3 = pd.concat([temp1, temp2], axis=0)\n",
    "\n",
    "temp = temp.merge(temp3, on='fsym_id', how='left')\n",
    "\n",
    "# get max pds\n",
    "temp4 = pd.read_csv('/Users/joeybortfeld/Downloads/max_pd_by_fsym.csv')\n",
    "temp = temp.merge(temp4, on='fsym_id', how='left')\n",
    "temp.to_csv('/Users/joeybortfeld/Downloads/download_status.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices_files = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/')]\n",
    "\n",
    "# # 0. get start price date per each fsym\n",
    "# prices_starts_dict = {}\n",
    "# for f in tqdm.tqdm(prices_files):\n",
    "#     df = pd.read_csv(f'/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/{f}.csv')\n",
    "#     df = df[df['price'].notnull()]\n",
    "#     df = df[df['price'] != 0]\n",
    "#     start_date = df['date'].min()\n",
    "#     prices_starts_dict[f] = start_date\n",
    "\n",
    "# print(len(prices_starts_dict))\n",
    "# fasdfasdf    \n",
    "\n",
    "# 1. get shares data\n",
    "response = factset_api.batch_get_shares_outanding(fsym_list=prices_files, \n",
    "                                  end_date='2024-12-31', \n",
    "                                  start_date_dict=prices_starts_dict,\n",
    "                                  frequency='M',\n",
    "                                  verbose=True,\n",
    "                                  authorization=credentials.factset_api_authorization,\n",
    "                                  skip_if_done=True,\n",
    "                                  output_folder='/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/shares/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fsyms = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices UNSPLIT/')]\n",
    "\n",
    "\n",
    "\n",
    "response = factset_api.batch_get_stock_data(metric='prices', \n",
    "                                fsym_list=price_fsyms, \n",
    "                                  start_date='2006-01-03', \n",
    "                                  end_date='2024-12-31', \n",
    "                                  frequency='D',\n",
    "                                  verbose=True,\n",
    "                                  authorization=credentials.factset_api_authorization,\n",
    "                                  skip_if_done=True,\n",
    "                                  output_folder='/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices SPLIT/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_starts_dict['PHS9MZ-R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify files in returns that are not in prices and vice versa\n",
    "returns_files = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/returns/')]\n",
    "prices_files = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/')]\n",
    "\n",
    "print(len(set(returns_files) - set(prices_files)))\n",
    "print(len(set(prices_files) - set(returns_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, temp = factset_api.get_shares_outanding(id_list=['K5JZYK-R'], \n",
    "                     start_date='2006-03-31', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='D',\n",
    "                     verbose=False,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "\n",
    "print(response)\n",
    "temp[temp['totalOutstanding'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp['publicationDate'].notnull()]['publicationDate'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, df = get_stock_prices(id_list=['CGF31Z-R'], \n",
    "                     field_list=[\"price\", \"volume\", \"tradeCount\"], \n",
    "                     start_date='2023-01-03', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='D',\n",
    "                     split='SPLIT',\n",
    "                     verbose=False,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "\n",
    "print(status)\n",
    "print(df.columns)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, df = get_stock_returns(id_list=['CGF31Z-R'], \n",
    "                    \n",
    "                     start_date='2006-01-03', \n",
    "                     end_date='2024-12-31', \n",
    "                     frequency='D',\n",
    "                     verbose=False,\n",
    "                     authorization=credentials.factset_api_authorization)\n",
    "print(status)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_files = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/returns/')]\n",
    "prices_files = [i.split('.')[0] for i in os.listdir('/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/')]\n",
    "\n",
    "returns_files.remove(\"\")\n",
    "\n",
    "\n",
    "# collect returns data\n",
    "collection = []\n",
    "for f in tqdm.tqdm(returns_files):\n",
    "    df = pd.read_csv(f'/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/returns/{f}.csv')\n",
    "    df = df[df['totalReturn'] != 0]\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    count = df['totalReturn'].count()\n",
    "    \n",
    "    collection.append([f, min_date, max_date, count])\n",
    "\n",
    "df1 = pd.DataFrame(collection, columns=['fsym_id', 'min_date_returns', 'max_date_returns', 'count_returns'])\n",
    "\n",
    "# collect prices data\n",
    "collection = []\n",
    "for f in tqdm.tqdm(prices_files):\n",
    "    df = pd.read_csv(f'/Users/joeybortfeld/Documents/QML Solutions Data/factset_data/factset_equity/prices/{f}.csv')\n",
    "    df = df[df['price'] != 0]\n",
    "    df = df[df['price'].notnull()]\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    count = df['price'].count()\n",
    "    \n",
    "    collection.append([f, min_date, max_date, count])\n",
    "\n",
    "df2 = pd.DataFrame(collection, columns=['fsym_id', 'min_date_prices', 'max_date_prices', 'count_prices'])\n",
    "\n",
    "df = df1.merge(df2, on='fsym_id', how='left')\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
